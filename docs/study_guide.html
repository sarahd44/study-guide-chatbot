<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>STA2023 Study Guide</title>
    <link rel="stylesheet" href="https://smokinnotes.s3.us-east-1.amazonaws.com/studyguide/sn25-v2.css">
    <script>
    MathJax = {
        tex: {
            displayMath: [['\\[','\\]']],
            inlineMath: [['\\(','\\)']]
        }
    };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script src="https://smokinnotes.s3.us-east-1.amazonaws.com/studyguide/sn25-v2.js"></script>
</head>
<body>

<h1>How to Do Well in STA2023</h1>

<p>STA2023 provides a basic introduction to statistical concepts and methods. Your grade in Prof. Ripol's course will be based on 3 exams (each worth 25% of your grade), 10 labs (12.5%), and 36 online quizzes (12.5%). Exams and labs are administered in person.</p>

<p>Each exam will consist of 33 multiple-choice questions (worth 3 points each) and 1 point for bringing a photo ID and bubbling in your name, UFID, and test code correctly. For exams, you are allowed a scientific calculator and a blank sheet of paper (formula sheets and reference tables will be provided). The exam dates are as follows:</p>

<table style="margin: 20px auto;">
<tr style="background-color: #8B0000; color: white;">
<td><strong>Exam 1</strong></td>
<td><strong>Wednesday, October 1<sup>st</sup> 8:20 p.m.</strong></td>
</tr>
<tr>
<td><strong>Exam 2</strong></td>
<td><strong>Tuesday, November 4<sup>th</sup>, 8:20 p.m.</strong></td>
</tr>
<tr>
<td><strong>Exam 3</strong></td>
<td><strong>Tuesday, December 9<sup>th</sup>, 5:30 p.m.</strong></td>
</tr>
</table>

<p>Here are some of our top tips for success in STA2023:</p>

<ul>
<li><strong>Try to answer quiz questions from memory.</strong> You might be tempted to try to find the answers to your quizzes online, but you'll learn the material better if you read the notes and then try to answer the questions on your own first.</li>

<li><strong>Start out strong.</strong> Most students who take STA2023 have learned at least some statistics in high school math classes, and you'll see several topics that you already know during the first part of the course. <em>Don't let this give you a false sense of security!</em> The course gets more difficult at the end of the Exam 1 material, when you learn about probability distributions. Students often find the probability chapters challenging, and it's even more challenging that they are taught right before your first exam.</li>

<li><strong>Don't scale back after Exam 1.</strong> If you do well on Exam 1, don't reduce your study efforts for Exam 2! Exam 2 tends to be the most difficult, as there is a lot of material, and you probably haven't seen most of it before. Also, the final part of the course applies many of the same concepts from the second part of the course, so it's important that you have a good understanding of the Exam 2 material.</li>

<li><strong>Take a few minutes each Sunday to take the SN progress quizzes on the week's material.</strong> Use them to reinforce your knowledge of the material and to identify which sections you might need to review. Then take at least 2 full Smokin'Notes practice exams in the week before the exam. Also, go through the questions from previous semesters' exams, which are posted on Canvas, and use the SN Detailed Solutions to go over your answers.</li>
</ul>

<p style="text-align: center; font-style: italic;"><strong>Good luck this semester! You'll do great!</strong></p>

<p class="date">Friday, 8/22/25</p>

<h1>Statistics Overview</h1>

<p>Statistics is the study of the collection, organization, and interpretation of data. It is both an art and a science. Particularly at the beginning of the course, we will discuss statistics as an art. For example, describing data using charts and graphs requires an artistic approach. Statistics is not about memorizing numbers; it is about using statistical methods to collect, organize, and analyze data to help us make decisions, and it is used by individuals, firms, and organizations in nearly every sector of the economy. Statistics—the art and science of learning from data—is divided into 3 basic areas:</p>

<ul>
<li><strong>Design</strong> is the process of determining how to collect the data. This involves activities such as selecting participants, writing surveys, setting up controls, and giving placebos.</li>

<li><strong>Description</strong> is the process of organizing the data in a meaningful way, usually with numerical summaries or graphs.</li>

<li><strong>Inference</strong> involves making some conclusion about the population based on a sample. Based on the (random, representative) sample, we might infer something about the entire population of interest. The 2 basic types of statistical inference are as follows:
    <ul>
    <li><strong>Confidence intervals</strong> allow us to come up with an interval and have a certain level of confidence that the true value in the population of interest is within that interval.</li>
    <li><strong>Significance tests</strong> allow us to use data to determine whether an observed difference (such as between 2 groups) is statistically significant or likely due to chance.</li>
    </ul>
</li>
</ul>

<p>Note that the "description" function of statistics stops with the sample; it does not make inferences about the population. The "inference" function is what really interests us, and it is what we will spend the most time on, particularly toward the end of the course.</p>

<h1>Using Samples to Learn about Populations</h1>

<p>This course will go over the collection, description, and analysis of data, which consist of:</p>

<ul>
<li><strong>Subjects</strong> are sources from which we collect data. They are usually people, but they can also be animals or objects.</li>

<li><strong>Variables</strong> are what we measure. For example, we might measure the weight or height of people, the time it takes a mouse to finish a maze, the strength of a fiber, or the return on a stock. These values may change from subject to subject; they are "vary"-able.</li>
</ul>

<p>The <strong>population</strong> refers to all of the subjects we are interested in, and a <strong>sample</strong> refers to all of the subjects for whom we have data. We are typically unable to collect data from the entire population because it would be too costly or time-consuming. As a result, we collect data from a sample and use it to make inferences about the population. This sample data is only meaningful if the sample is representative of the population (which random sampling helps to achieve).</p>

<p>The differences between samples and populations are key in the distinction between statistics and parameters.</p>

<ul>
<li><strong>Statistics</strong> – A <strong>statistic</strong> is a numerical summary of the data for a <em>sample</em>.</li>
<li><strong>Parameters</strong> – A <strong>parameter</strong> is a numerical summary of the data for a <em>population</em>.</li>
</ul>

<p>The fact that we rarely have data for the entire population means that we will rarely <em>know</em> the parameter. Rather, our best ideas about the parameter will be based on inference from the sample. In certain rare cases, however, we can get data for the entire population. When we are able to do this, we don't need to use statistical inference; description is all that's required.</p>

<div class="example-box">
<h3>Example: Handedness</h3>

<p><strong>Studies by psychologists have indicated that 87% of Americans are right-handed. Suppose that Prof. Ripol is interested in determining whether this is true for students at UF. During an assembly exam, she walks around and counts how many students are writing with their right hands. She finds that, out of the 100 students in the class, 85 of them write with their right hands.</strong></p>

<p>The following is a description of each of the important statistical aspects of this scenario:</p>

<ul>
<li>The <strong>variable</strong> in this case is handedness—which hand is dominant.</li>
<li>The <strong>population</strong> is all of the students at UF.</li>
<li>The <strong>sample</strong> is the 100 students in the exam room when she counts.</li>
<li>The <strong>parameter</strong> is the share of <em>all</em> UF students who are right-handed, which is unknown.</li>
<li>The <strong>statistic</strong> is the share of students in the exam room who are right-handed. In this case, 85/100, or 85% of the sample, was right-handed.</li>
</ul>

<p>Random sampling was <strong>not</strong> used in this example because not every UF student has to take statistics, so not every UF student had an equal chance of being selected for the sample. People who are right-handed and left-handed may be gifted in certain areas, so they may have different academic interests. Right-handed people may be more or less likely than left-handed people to take statistics. This would bias the sample, illustrating why convenience samples like this scenario are typically not representative of the population of interest.</p>
</div>

<p class="date">Monday, 8/25/25</p>

<h1>Types of Data</h1>

<p>Data can come from a number of different types of variables:</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-01-datatypeshierarchy.png" 
     alt="Hierarchical diagram showing data types: Data branches into Categorical Variables and Quantitative Variables, with examples and further subdivisions" 
     class="image-right-40" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<ul>
<li><strong>Categorical variables</strong> do not take on meaningful numerical values. As a result, observations are generally put into groups and summarized as a percentage. Things like gender, race, favorite color, and major are all categorical.

<p><em>Be careful!</em> It would be incorrect to say that quantitative variables involve numbers and categorical variables do not; categorical variables can include numbers as well. For example, area codes and zip codes are numerical, but they are not quantitative variables because they do not take on meaningful numerical values. These numbers are just labels, or "identifiers."</p>

<p>A good test for determining whether a numerical value is a categorical variable or a quantitative variable is to ask yourself if the number can be meaningfully averaged. If it cannot be meaningfully averaged, it is a categorical variable. For instance, you <em>could</em> take the average of all of the zip codes of a group of people, but the average wouldn't mean anything, so zip codes are categorical.</p>
</li>

<li><strong>Quantitative variables</strong> take on meaningful numerical values, which can be described in terms of <em>center</em> and <em>spread</em>. There are 2 types of quantitative variables:
<ul>
<li><strong>Discrete variables</strong> can take on a finite list of outcomes—usually whole numbers. When we are <em>counting</em> something, we are usually dealing with discrete variables. For example, number of free throws made, number of blocked shots, and number of broken eggs in a carton are all discrete variables. Although discrete variables <em>usually</em> take on whole numbers, they don't <em>always</em> have to. For example, you could count by parts (such as halves) as well.</li>

<li><strong>Continuous variables</strong> can take on an infinite list of outcomes. These data may take on variables within an interval, but there is an infinite number of possibilities within that interval. Height, weight, and GPA are examples of continuous variables. In general, when values can have many decimal places, you are dealing with a continuous variable. For example, your height could be 60 inches, 60.1 inches, 60.11 inches, etc.; there are an infinite number of possibilities.</li>
</ul>
</li>
</ul>

<h1>Using Graphical Summaries to Describe Data</h1>

<p>The appropriate type of graphical summary depends on the type of variable being considered:</p>
<ul>
<li><em>Categorical variables</em> – Bar charts and pie charts are appropriate for summarizing categorical data.</li>
<li><em>Quantitative variables</em> – Dotplots, histograms, and stem-and-leaf plots (or stemplots) are appropriate for summarizing quantitative data.</li>
</ul>

<h2>Graphical Summaries for Categorical Variables</h2>

<p>Recall that categorical variables are generally described using percentages. Categorical variables can be summarized with bar charts and pie charts.</p>

<ul>
<li>A <strong>bar chart</strong> uses bars with lengths proportional to percentages for each variable. For example, suppose a professor collects the following data by asking students to indicate whether they are freshmen, sophomores, juniors, or seniors:

<table>
<thead>
<tr>
<th>Year in College</th>
<th>Frequency<br>(Count)</th>
<th>Proportion</th>
<th>Percentage<br>(Relative<br>Frequency)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Freshman</td>
<td>207</td>
<td>\(\frac{207}{278} = 0.7446\)</td>
<td>74.46%</td>
</tr>
<tr>
<td>Sophomore</td>
<td>57</td>
<td>\(\frac{57}{278} = 0.2050\)</td>
<td>20.5%</td>
</tr>
<tr>
<td>Junior</td>
<td>9</td>
<td>\(\frac{9}{278} = 0.0324\)</td>
<td>3.24%</td>
</tr>
<tr>
<td>Senior</td>
<td>5</td>
<td>\(\frac{5}{278} = 0.0180\)</td>
<td>1.8%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>278</strong></td>
<td><strong>≈1.0</strong></td>
<td><strong>≈100%</strong></td>
</tr>
</tbody>
</table>

<p>Note that the sum of the proportions must add to 1.00 because we have accounted for all of the possibilities. That is, there were no students who were not freshmen, sophomores, juniors, or seniors. (In some cases, if the proportions do not <em>exactly</em> equal 1.00, it may be because of a rounding error.) </p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-02-barchartcollege.png" 
     alt="Bar chart showing year in college distribution with Freshman at 74.46%, Sophomore at 20.50%, Junior at 3.24%, and Senior at 1.80%" 
     class="image-wide-50" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>By convention, the bars in a bar chart (for categorical variables) don't touch one another, while the bars in a histogram (for quantitative variables) do. Additionally, because we are using a categorical variable on the <em>x</em>-axis, we wouldn't analyze the shape (skewed, bell-shaped, etc.) of a bar chart.</p>
</li>

<li>
A <strong>pie chart</strong> uses a circle divided into wedges that represent proportions of the whole. The entire circle represents 100%, and the various "slices" of the pie represent the various percentages.
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-03-piechartcollege.png" 
     alt="Pie chart showing college year distribution with 74% Freshman, 21% Sophomore, 3% Junior, and 2% Senior" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />
</li>
</ul>

<h2>Graphical Summaries for Quantitative Variables</h2>

<p>Quantitative variables—both discrete and continuous—can be summarized with dotplots, histograms, and stem-and-leaf plots. These types of graphs allow us to make certain conclusions about the center, spread, and shape of the distribution. Throughout this section, we will use the following data set of exam scores in a small class of 11 students:</p>

<p><strong>72, 84, 91, 73, 76, 83, 84, 92, 68, 65, 76</strong></p>

<ul>
<li>A <strong>dotplot</strong> is a number line with a dot for every observation. If a value occurs more than once, the dots are stacked. A dotplot for our exam score data set might look something like this:

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-04-dotplotexamscores.png" 
     alt="Dotplot showing exam scores from 60 to 100 with stacked dots at values, showing two dots at 76 and 84" 
     class="image-wide-70" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>Note that there are two stacked dots for the values of 76 and 84. This indicates that those values occur twice in the data set.</p>
</li>

<li>A <strong>histogram</strong> is a diagram with bars whose lengths are proportional to the frequency of a variable within a specified interval. It is like a dotplot that groups observations into intervals, and we use bars instead of dots.

<p>The vertical axis can have the count (frequency), percent, or proportion. In this example, we will simply use frequency, and we will group the observations together in ranges from 61 to 70, 71 to 80, 81 to 90, and 91 to 100.</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-05-histogramexamgrades.png" 
     alt="Histogram showing exam grades with frequency on y-axis and exam grades ranges on x-axis, bars at intervals 60-70, 71-80, 81-90, 91-100" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />

</li>

<li>A <strong>stemplot</strong> (also called a <em>stem-and-leaf plot</em>) is a method of graphically presenting quantitative data in a way that assists in visualizing the shape of the distribution. A stemplot is essentially a histogram turned on its side. One advantage of stemplots is that they always show the exact values for all of the observations, making them the most detailed type of graph for quantitative data.

<p>The stemplot is divided into two portions: a <strong>stem</strong> and a <strong>leaf</strong>. The stem is always displayed to the left of the leaf, and each leaf always consists of one digit. Each observation is added to the plot by un-pairing the last digit of the observation (the leaf) from the rest (the stem). When we read from a stemplot, we put each leaf back together with its stem and we multiply the result by the <strong>leaf unit</strong>.</p>
</li>


<p>A stemplot for our exam grade data is as follows:</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-06-stemplotexample.png" 
     alt="Stemplot showing stems 6,7,8,9 with leaves 58, 2366, 344, 12 respectively, with leaf unit = 1.0" 
     class="image-wide-20" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>If we were to read the stemplot, we would pair the stems up with the leaves and multiply by the leaf unit. For example, the first observation listed has a stem of "6" and a leaf of "5." Therefore, this observation is 65 × 1.00 = 65. If the leaf unit were 0.10, the observation would be 65 × 0.10 = 6.5. If the leaf unit were 10.0, the observation would be 65 × 10 = 650.</p>
</ul>

<h2>Common Shapes of Distributions</h2>

<p>Data can take on a number of different kinds of distribution shapes. You should know what these various shapes are. Some of the most common kinds of distributions are as follows:</p>

<ul>
<li>A distribution is <strong>mound or bell-shaped</strong> when most of the values are concentrated in the middle of the distribution, and the values decrease as we move above or below the center. This kind of distribution is <strong>unimodal</strong> (meaning it has one mode) and <strong>roughly symmetric</strong> (meaning that the portions to the left and the right of center are <em>roughly</em> mirror images of one another).
<p></p>
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-07-bellshapeddist.png" 
     alt="Bell-shaped distribution histogram showing symmetric mound shape with example weight, height" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>The bell-shaped distribution is the most common type of distribution that we will discuss in this class. Examples of variables that we expect to show this kind of distribution include human height and weight.</p>
</li>

<li>A <strong>uniform</strong> or <strong>rectangular distribution</strong> is one in which all of the options are equally likely. As a result, the histogram is shaped like a rectangle.
<p></p>
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-08-uniformdist.png" 
     alt="Uniform distribution histogram showing equal height rectangular bars" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>For example, suppose you rolled fair, six-sided dice over and over again. Because the odds of getting each number are the same (1/6), you would expect that a histogram showing the frequency of each number would take on a uniform distribution.</p>
</li>

<li>A <strong>bimodal distribution</strong> has two modes, which are represented by two big bumps in the associated histogram. A bimodal distribution usually results when we combine data from two groups or when opinions about a topic are highly polarized.
<p></p>
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-09-bimodaldist.png" 
     alt="Bimodal distribution histogram showing two distinct peaks with example opinions of Republicans and Democrats" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />
</li>

<li>A distribution is <strong>"skewed left"</strong> if the histogram has a tail extending to the left (i.e., towards the lower numbers), and most of the observations are clustered together around a higher number.
<p></p>
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-10-leftskeweddist.png" 
     alt="Left-skewed distribution histogram with tail extending left and example grades on an easy exam" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>An example is grades on an easy exam. Most students score well on easy exams, but they cannot get more than 100 points. Therefore, the distribution will be skewed left, with most students receiving a high score and few students receiving low scores.</p>
</li>

<li>A distribution is <strong>"skewed right"</strong> if the histogram has a tail extending to the right (i.e., towards the higher numbers), and most of the observations are clustered together around a lower number.
<p></p>
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-11-rightskeweddist.png" 
     alt="Right-skewed distribution histogram with tail extending right and example incomes, house prices" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>Examples include income and house prices. There are a few extremely rich people who have extremely high incomes and extremely expensive houses. These people form the "tail" to the right of the cluster of most peoples' incomes and house prices.</p>
</li>
</ul>

<p>Now that we have described various ways to summarize data using graphs, we will discuss ways to describe two important characteristics of a distribution: <em>center</em> and <em>spread</em>.</p>

<p class="date">Wednesday, 8/27/25</p>

<h1>Describing the Center of Quantitative Data</h1>

<p>We will explore three ways to describe the <strong>center</strong>: mean (\(\bar{x}\)), median (<em>M</em>), and mode. To illustrate these measures of center, we will use the following data sets, which reflect how many photos a sample of male and female college students have posted on Instagram:</p>

<p><strong>Females:</strong> <b>88, 106, 243, 65, 99, 105, 120, 128, 140</b></p>
<p><strong>Males:</strong> <b>54, 86, 1, 33, 87, 66, 54, 59, 69, 103</b></p>

<h2>Mean (\(\bar{x}\))</h2>

<p>The <strong>mean</strong> (which is denoted by \(\bar{x}\)) is the average of all of the observations. We get the mean by adding up all of the observations and dividing by <em>n</em>, the number of observations in the data set.</p>

\[\bar{x} = \frac{\sum x}{n} = \frac{x_1 + x_2 + x_3 + ... + x_n}{n}\]

<p>Note that the Greek letter capital sigma (Σ) is the summation notation (it is simply shorthand for "the sum of"), <em>x</em> is the notation for a specific observation, and <em>n</em> notates the number of observations. Here, the variables <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, and <em>x</em><sub>3</sub> refer to the first, second, and third observations in the data set, respectively; <em>x<sub>n</sub></em> refers to the last observation in the data set.</p>

<p>Let's calculate the mean for our Instagram photo data sets:</p>

\[\bar{x}_{Females} = \frac{\sum x_i}{n} = \frac{88+106+243+65+99+105+120+128+140}{9} = \frac{1,094}{9} = 121.6\]

\[\bar{x}_{Males} = \frac{\sum x_i}{n} = \frac{54+86+1+33+87+66+54+59+69+103}{10} = \frac{612}{10} = 61.2\]

<h2>Median (<em>M</em>)</h2>

<p>The <strong>median</strong> (which is denoted by <em>M</em>) is the observation in the middle of the ordered data set. To determine the median, it is <em>crucial</em> that you put the data in numerical order. Let's put our Instagram photo data sets in numerical order and then find the medians:</p>

<p><strong>Females:</strong> 65, 88, 99, 105, <strong>106</strong>, 120, 128, 140, 243</p>

<p>The middle value for females, 106, is easy to identify; there are four numbers before it and four numbers after it. One of your observations will always be the median when you have an odd number of observations.</p>

<p><strong>Males:</strong> 1, 33, 54, 54, <strong>59, 66</strong>, 69, 86, 87, 103</p>

<p>The male data set has an even number of observations, so there is no single middle observation. This will always be the case when you have an even number of observations in your data set. When this happens, we simply take the average of the two middle values:</p>

\[M_{Males} = \frac{59+66}{2} = \frac{125}{2} = 62.5\]

<p>In these examples, we could easily identify the middle values by counting from the beginning or end of the data set. However, if we had a data set with hundreds or thousands of observations, it would be impractical to count them, so we can use the <strong>position formula</strong> to determine the position of the median in the data set:</p>

\[\text{Position of Median} = \frac{n+1}{2}\]

<div class="exam-tip">
<h4>EXAM TIP: Position Formula</h4>
<p>The position formula should be memorized, as it may not be provided on the exam.</p>
</div>

<p>For example, our female data set had a total of nine observations, so the median was in the (9 + 1) ÷ 2 = <strong>5<sup>th</sup></strong> position. The male data set had a total of 10 observations, so the median was in the (10 + 1) ÷ 2 = 5.5th position, meaning it was between the 5th and 6th position.</p>

<h2>Mode</h2>

<p>The <strong>mode</strong> is the most common observation in the data set. The mode is a particularly important measure of center when we are dealing with <em>bimodal distributions</em>—distributions with two modes. To find the mode, it is useful to put the data set in numerical order so that you can identify repeated values. In the females' data set, there were no repeating values. Because each value occurs once, there is no mode for this data set. In the males' data set, however, two males had 54 photos on Instagram. Therefore, <strong>54</strong> is the mode for the male data set.</p>

<h1>Describing the Spread of Quantitative Data</h1>

<p>The <strong>spread</strong> describes how squished in or dispersed the data are. Three measures of spread (also known as <em>measures of variability</em> or <em>measures of dispersion</em>) are the range, variance, and standard deviation.</p>

<h2>Range</h2>

<p>The <strong>range</strong> is the difference between the maximum and minimum values in a data set.</p>

\[\text{Range} = \text{Maximum value} - \text{Minimum value}\]

<p>Note that the range is a <em>single</em> number, not an actual range. That is, you cannot say, "The range is between 20 and 30." You have to actually do the subtraction and say, "The range is 10."</p>

<p>Let's calculate the range for our Instagram photos data sets:</p>

<p><strong>Females:</strong> Range = 243 − 65 = <strong>178</strong></p>
<p><strong>Males:</strong> Range = 103 − 1 = <strong>102</strong></p>

<h2>Variance</h2>

<p>The <strong>variance</strong> (<em>s</em><sup>2</sup>) is the average of the squared deviations from the mean.</p>

\[s^2 = \frac{\sum (x - \bar{x})^2}{n-1}\]

<p>Observations below the mean will have a negative distance to the mean, and observations above the mean will have a positive distance. By definition, the mean is the point where these negative and positive distances balance. Therefore, the deviations from the mean for all of the observations in a data set will necessarily sum to zero. To get rid of positive and negative values that would cancel each other out, the deviations must be squared.</p>

<p>Because we are measuring the <em>squared</em> distance of each observation from the mean, the variance will be in <em>squared</em> units. For example, if we are talking about height, the variance may be in inches squared, and if we are talking about temperature, the variance may be in "degrees Celsius squared."</p>

<h2>Standard Deviation (<em>s</em>)</h2>

<p>The <strong>standard deviation</strong> is the square root of the variance:</p>

\[s = \sqrt{s^2} = \sqrt{\frac{\sum (x - \bar{x})^2}{n-1}}\]

<p>Whereas the variance is expressed in <em>squared</em> units, the standard deviation is expressed in <em>linear</em> units—ones we can easily interpret. For example, the units used for the standard deviation might be degrees Celsius (whereas the units for the variance were degrees Celsius squared).</p>

<p>Calculating variances and standard deviations by hand is tedious and unnecessary because almost all calculators have a function that will find the standard deviation for you. Nevertheless, we will show you how to find the standard deviation for two sample data sets below, creating a table showing the deviations from the mean and the squared deviations:</p>

<p><strong>Data Set A:</strong><b> 2, 2, 2, 5, 8, 8, 8</b></p>

\[\bar{x} = \frac{2+2+2+5+8+8+8}{7} = \frac{35}{7} = 5\]

<table>
<thead>
<tr>
<th><em>x</em></th>
<th>\(\bar{x}\)</th>
<th>(<em>x</em> – \(\bar{x}\))</th>
<th>(<em>x</em> – \(\bar{x}\))<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>5</td>
<td>−3</td>
<td>9</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>−3</td>
<td>9</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>−3</td>
<td>9</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>8</td>
<td>5</td>
<td>3</td>
<td>9</td>
</tr>
<tr>
<td>8</td>
<td>5</td>
<td>3</td>
<td>9</td>
</tr>
<tr>
<td>8</td>
<td>5</td>
<td>3</td>
<td>9</td>
</tr>
<tr>
<td><strong>TOTAL:</strong></td>
<td></td>
<td><strong>0</strong></td>
<td><strong>54</strong></td>
</tr>
</tbody>
</table>

\[\text{Variance} = \frac{\sum(x - \bar{x})^2}{n-1} = \frac{54}{7-1} = 9\]

\[\text{Standard Deviation} = \sqrt{s^2} = \sqrt{9} = 3.0\]

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-12-dotplotdataseta.png" 
     alt="Dotplot for Data Set A showing values 2, 3, 4, 5, 6, 7, 8 with dots stacked at 2, 5, and 8" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p><strong>Data Set B:</strong><b> 2, 4, 5, 5, 5, 6, 8</b></p>

\[\bar{x} = \frac{2+4+5+5+5+6+8}{7} = \frac{35}{7} = 5\]

<table>
<thead>
<tr>
<th><em>x</em></th>
<th>\(\bar{x}\)</th>
<th>(<em>x</em> – \(\bar{x}\))</th>
<th>(<em>x</em> – \(\bar{x}\))<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>5</td>
<td>−3</td>
<td>9</td>
</tr>
<tr>
<td>4</td>
<td>5</td>
<td>−1</td>
<td>1</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>6</td>
<td>5</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>8</td>
<td>5</td>
<td>3</td>
<td>9</td>
</tr>
<tr>
<td><strong>TOTAL:</strong></td>
<td></td>
<td><strong>0</strong></td>
<td><strong>20</strong></td>
</tr>
</tbody>
</table>

\[\text{Variance} = \frac{\sum(x - \bar{x})^2}{n-1} = \frac{20}{7-1} = 3.33\]

\[\text{Standard Deviation} = \sqrt{s^2} = \sqrt{3.33} = 1.83\]

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-13-dotplotdatasetb.png" 
     alt="Dotplot for Data Set B showing values 2, 3, 4, 5, 6, 7, 8 with single dots at most values and multiple dots at 5" 
     class="image-wide-35" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>For the two example data sets above, notice that the variance and standard deviation are smaller for the data set with more of the values clustered together around the mean (Data Set B).</p>

<h2>Interpreting Standard Deviation</h2>

<p>The larger the standard deviation, the more spread out the data. Note the following characteristics of the standard deviation:</p>

<ul>
<li><strong>The standard deviation can never be negative.</strong> We get the standard deviation by taking the square root of the variance, and we get the variance by averaging the sum of the squared differences between each observation and the mean. As a result, neither the standard deviation nor the variance can be negative.</li>

<li><strong>The standard deviation is rarely zero.</strong> A standard deviation of zero means that the numbers in the data set are not spread out at all. We would only get a standard deviation of zero if all of the numbers in the data set were the same (e.g., a data set of 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1). This data set has a variance of zero and a standard deviation of zero.</li>

<li><strong>The standard deviation is affected by outliers.</strong> The standard deviation is not as heavily influenced by outliers as the range, but it is still affected by outliers. A significant outlier has a large distance from the mean, which increases the variance and standard deviation.</li>

<li><strong>The standard deviation is most useful when we're dealing with bell-shaped and symmetric distributions.</strong> We can use the <strong>empirical rule</strong>, which approximates very convenient properties of distributions that are both <strong>bell-shaped</strong> and <strong>symmetric</strong>.

<p>Sometimes referred to as the <strong>68–95–99.7 rule</strong>, the empirical rule states the following:</p>
<ul>
<li>68% of observations fall within ±1 standard deviations from the mean.</li>
<li>95% of observations fall within ±2 standard deviations from the mean.</li>
<li>99.7% of observations fall within ±3 standard deviations from the mean.</li>
</ul>

<p>If the area below a normal curve sums to 100%, we could show the empirical rule in a series of diagrams as follows:</p>

<style>
.empirical-rule-container {
    display: flex !important;
    flex-wrap: wrap !important;
    justify-content: center !important;
    gap: 10px !important;
    margin: 20px 0 !important;
}

.empirical-rule-container img {
    flex: 1 1 300px !important;
    max-width: 350px !important;
    height: auto !important;
    display: block !important;
}

@media (max-width: 1024px) {
    .empirical-rule-container img {
        flex: 1 1 400px !important;
        max-width: 500px !important;
    }
}

@media (max-width: 768px) {
    .empirical-rule-container {
        flex-direction: column !important;
    }
    .empirical-rule-container img {
        flex: none !important;
        max-width: 100% !important;
    }
}
</style>

<div class="empirical-rule-container">
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-14a-empiricalrule68.png" 
     alt="Normal distribution curve showing 68% of observations fall within 1 standard deviation of the mean" 
     class="image-wide-32" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-14b-empiricalrule95.png" 
     alt="Normal distribution curve showing 95% of observations fall within 2 standard deviations of the mean" 
     class="image-wide-32" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-14c-empiricalrule997.png" 
     alt="Normal distribution curve showing 99.7% of observations fall within 3 standard deviations of the mean" 
     class="image-wide-32" 
     loading="lazy" 
     width="auto" 
     height="auto" />
</div>
</li>
</ul>

<div class="other-box">
<h4>Finding Standard Deviation</h4>
<p>Instructions on how to calculate standard deviation using four common scientific calculators (the Casio fx-115MS, TI-30Xa, TI-30XIIS, and TI-36X) can be found at tinyurl.com/calculatorinfo</p>
<p>You are responsible for knowing how to operate your calculator, and you may be asked to calculate a standard deviation with it on this exam and subsequent exams this semester. Try using your calculator to calculate the standard deviation of Data Set B above: 2, 4, 5, 5, 5, 6, 8. You should find that the standard deviation is <strong>1.8257</strong>.</p>
</div>

<p class="date">Friday, 8/29/25</p>

<h1>Using Measures of Position to Describe Spread</h1>

<p>Now that we have discussed three measures of the spread—the range, variance, and standard deviation—we will turn to ways in which we can use measures of <strong>position</strong> to describe the spread. You have probably been exposed to percentiles before in the context of standardized testing. For example, if someone is in the 98th percentile, 98% of the people who took the test scored below that person. This is a measure of position.</p>

<h2>Dividing the Data into Quartiles</h2>

<p>Just as we can divide football games into quarters, we can divide a data set into <strong>quartiles</strong>. (Note that a quarter in a football game refers to the entire period, whereas a quartile refers to the absolute end of an interval.) In general, we can divide the data set with three quartiles: Q<sub>1</sub>, Q<sub>2</sub>, and Q<sub>3</sub>.</p>

<ul>
<li><strong>Q<sub>1</sub></strong> – The <strong>lower quartile</strong> (Q<sub>L</sub>) is the 25th percentile—the median of the lower half of the data. This is the value for which 25% of the data is lower.</li>
<li><strong>Q<sub>2</sub></strong> – The <strong>median</strong> (<em>M</em>) is the 50th percentile—the median of the entire data set. This is the value for which 50% of the data is lower and 50% is higher.</li>
<li><strong>Q<sub>3</sub></strong> – The <strong>upper quartile</strong> (Q<sub>U</sub>) is the 75th percentile—the median of the upper half of the data. This is the value for which 75% of the data is lower.</li>
</ul>

<p>To illustrate how we divide data into quartiles, we'll use the data sets reflecting the number of photos males and females have on Instagram. These data sets are in numerical order below:</p>

<p><strong>Females:</strong><b> 65, 88, 99, 105, 106, 120, 128, 140, 243</b></p>
<p><strong>Males:</strong><b> 1, 33, 54, 54, 59, 66, 69, 86, 87, 103</b></p>

<p>Recall that the median of the female data set was 106 and that the median of the male data set was 62.5. These are our values for Q<sub>2</sub>.</p>

<h3>Determining Q<sub>1</sub> and Q<sub>3</sub> for Females</h3>

<p>To determine the lower quartile for the females, we need to find the median of the lower half of the data (i.e., the data below the median). For females, the median was 106. There are four observations below the median, so Q<sub>1</sub> is average of the two middle numbers of the lower half:</p>

<p><strong>Females:</strong> <u>65, 88, 99, 105</u>, 106, 120, 128, 140, 243</p>

\[Q_1 = M_{\text{Lower half}} = \frac{88 + 99}{2} = 93.5\]

<p>To determine the upper quartile for the females, we need to find the median of the upper half of the data (i.e., the data above the median). For females, the median was 106. There are four observations above the median, so Q<sub>3</sub> is the average of the upper half's middle two numbers:</p>

<p><strong>Females:</strong> 65, 88, 99, 105, 106, <u>120, 128, 140, 243</u></p>

\[Q_3 = M_{\text{Upper half}} = \frac{128 + 140}{2} = 134\]

<h3>Determining Q<sub>1</sub> and Q<sub>3</sub> for Males</h3>

<p>To determine the lower quartile for the males, we need to find the median of the lower half of the data (i.e., the data below the median). For males, the median was 62.5 (the average of the two middle numbers, 59 and 66). When we count the numbers below the median (62.5), we <em>include</em> 59. Therefore, Q<sub>1</sub> is the middle number of the lower half:</p>

<p><strong>Males:</strong> <u>1, 33, 54, 54, 59</u>, 66, 69, 86, 87, 103</p>

\[Q_1 = M_{\text{Lower half}} = 54\]

<p>To determine the upper quartile for the males, we need to find the median of the upper half of the data (i.e., the data above the median). Again, the median was 62.5—the average of 59 and 66. When we count the numbers above the median, we <em>include</em> the 66. Therefore, Q<sub>3</sub> is the middle number of the upper half:</p>

<p><strong>Males:</strong> 1, 33, 54, 54, 59, <u>66, 69, 86, 87, 103</u></p>

\[Q_3 = M_{\text{Upper half}} = 86\]

<h2>Describing Quartiles and the Interquartile Range (IQR)</h2>

<p>The <strong>interquartile range (IQR)</strong> is another measure of spread, which describes how spread out the central 50% of the data is. The interquartile range is the numerical difference between the upper quartile and the lower quartile. It is calculated as follows:</p>

\[\text{IQR} = Q_3 - Q_1\]

<p>Like the range, the IQR is calculated as a single number, not an interval. For example, it would not be correct to say that the IQR for the males was between 54 and 86 photos. It <em>would</em> be correct to say that the IQR was 32 photos (86 − 54). Likewise, you could calculate the IQR for the female data set to be 40.5 photos (134 − 93.5).</p>

<h1>Resistance to Outliers</h1>

<p>A measure is <strong>resistant to outliers</strong> when it is unaffected by the presence of an extremely small value or an extremely large value (i.e., an outlier). The <strong>median</strong> and <strong>mode</strong> are resistant to outliers, but the <strong>mean</strong> is NOT resistant to outliers.</p>

<div class="example-box">
<h4>Example: Mean Is Not Resistant to Outliers</h4>

<p><strong>Suppose that you poll students and ask them how much they spent on their lunch today. Your data set is as follows:</p>

<p>$3, $6, $6, $8, $12</strong></p>

<p>In this case, we could find that the mean is \(\bar{x} = (\$3 + \$6 + \$6 + \$8 + \$12) ÷ 5 = \$35 ÷ 5 = \$7\), the median is <strong>$6</strong>, and the mode is <strong>$6</strong>.</p>

<p><strong>Now, suppose that the person who spent $12 on lunch had a birthday that day, and his or her parents were in town. They went to Mark's Prime Steakhouse downtown and had a fine steak lunch that cost $47. The new data set would be as follows:</p>

<p>$3, $6, $6, $8, $47</strong></p>

<p>Here, <strong>$47</strong> is a clear outlier because it is so far from all of the other observations. Let's see how this outlier would affect the mean, median, and mode:</p>

<p>The mean is now \(\bar{x} = (\$3 + \$6 + \$6 + \$8 + \$47) ÷ 5 = \$70 ÷ 5 = \$14\), the median is <em>still</em> <strong>$6</strong>, and the mode is <em>still</em> <strong>$6</strong>. Changing the last number to an outlier did not affect the median or mode, but it <strong>doubled</strong> the mean. This is why we say the median and mode are resistant to outliers, but the mean is not; the mean gets pulled in the direction of an outlier.</p>
</div>

The range measures spread based on the maximum and minimum values, so it is highly influenced by outliers. The key advantage of the IQR over the range is that the IQR is <strong>resistant</strong> to outliers. This is because the IQR measures spread based on the upper and lower quartiles—which are, by definition, <em>not</em> the maximum or minimum value.</p>

<h2>Shape, Mean, Median, and Mode</h2>

<p>When a distribution is <em>symmetric</em> (whether unimodal or bimodal), the mean and the median are the same.</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-15-shaperelationships.png" 
     alt="Four distribution diagrams showing relationships between mean, median, and mode: Bell-shaped (unimodal) where mean=median=mode, Bimodal where mean=median with two separate modes, Skewed Left where mean < median < mode, and Skewed Right where mode < median < mean" 
     class="image-wide-90" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>In a <strong>bell-shaped</strong> distribution, all of the measures of center are the same, so we generally use the one that's the easiest to calculate: the <strong>mean</strong>. Because we use the mean, the preferred measure of spread for a bell-shaped distribution is <strong>standard deviation</strong>. This also applies for a uniform distribution. In a <strong>bimodal</strong> distribution, the two <strong>modes</strong> are the most representative measure of center (of each of the "humps"). In a distribution that is <strong>skewed</strong> (in either direction), the <strong>median</strong> is the most representative measure of center. To get the best picture of a bimodal or skewed distribution, we might report both the range and interquartile range (IQR) as measures of spread.

<h1>Boxplots</h1>

<p>The <strong>five-number summary of positions</strong> describes a data set with the following information:</p>
<ul>
<li>The minimum value</li>
<li>The lower quartile (Q<sub>1</sub>)</li>
<li>The median</li>
<li>The upper quartile (Q<sub>3</sub>)</li>
<li>The maximum value</li>
</ul>

<p>A <strong>boxplot</strong> (also called a <em>box-and-whisker plot</em>) is a graphical way to present the five-number summary for a data set.</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-16-basicboxplot.png" 
     alt="Basic boxplot diagram showing labeled components: minimum value, lower quartile Q1, median, upper quartile Q3, and maximum value with whiskers extending from box" 
     class="image-wide-60" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>For example, the following boxplot describes the data in our female Instagram photo data set:</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-17-femaleinstagrambox.png" 
     alt="Boxplot for female Instagram data showing scale from 60 to 240, with box and whiskers representing the five-number summary" 
     class="image-wide-100" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>The five-number summary that this boxplot displays is as follows:</p>
<ul>
<li>Minimum value = 65</li>
<li>Q<sub>1</sub> = Lower Quartile = 93.5</li>
<li>Q<sub>2</sub> = Median = 106</li>
<li>Q<sub>3</sub> = Upper Quartile = 134</li>
<li>Maximum value = 243</li>
</ul>

<p>The following are some important notes about boxplots:</p>

<ul>
<li><strong><em>The median is not always in the center of the box.</em></strong> You can use this to determine whether the distribution is skewed right, skewed left, or symmetric.
<ul>
<li>If the left side of the box is much smaller than the right side and the left whisker is much smaller than the right whisker, then the distribution is skewed right.</li>
<li>If the sides of the box and lengths of the whiskers are roughly equal, then the distribution is roughly symmetric.</li>
<li>If the right side of the box is much smaller than the left and the right whisker is much smaller than the left whisker, then the distribution is skewed left.</li>
</ul>
</li>

<li><strong><em>You cannot use boxplots to determine whether a distribution is bell-shaped (unimodal), or bimodal</em></strong> (you can only determine whether it is symmetric or skewed).</li>

<li><strong>A boxplot could be shown horizontally or vertically.</strong></li>
</ul>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-18-boxplotcomparisons.png" 
     alt="Comparison of distribution shapes with corresponding boxplots: Skewed Right, Symmetric, and Skewed Left distributions shown with both horizontal and vertical boxplot orientations" 
     class="image-wide-100" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<ul>
<li><strong><em>In Minitab, the whiskers do not extend to outliers.</em></strong> Above, we showed the "traditional" way to draw a boxplot, in which the whiskers extend from the box to the minimum and maximum values in the data set. Many software packages, including Minitab, alter this approach a bit by only extending the whiskers to the smallest and largest numbers that are <em>not outliers</em>. When this is the case, outliers are shown as asterisks (*).

<p>In our female Instagram photo data set, the "243" value is probably an outlier because it is so far above the rest of the numbers (65, 88, 99, 105, 106, 120, 128, 140, 243). Therefore, if we graph this data using Minitab, we get a boxplot that looks like this:</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-19-minitabboxplot.png" 
     alt="Minitab-style boxplot with asterisk showing outlier at 243, with whisker stopping at 140 (largest non-outlier value)" 
     class="image-wide-80" 
     loading="lazy" 
     width="auto" 
     height="auto" />
</li>
</ul>

<h2>Interpreting Stemplots in Minitab</h2>

<p>Stemplots produced by Minitab computer software are somewhat different. For example, consider the Minitab stemplot below, which shows the semester grade point averages for students enrolled in an economics course. As you can see, this stemplot has one more column than the one we looked at before. The leftmost column is a column that describes the <strong>cumulative count</strong> of observations. This column is in place to help you more easily find the median in this data set.</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter02-20-minitabstemplot.png" 
     alt="Minitab stemplot showing cumulative counts, stems, and leaves for GPA data with N=71, Leaf Unit=0.10, with line containing median marked with parentheses" 
     class="image-wide-70" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>The line with the parentheses is the line containing the median. The number inside the parentheses simply denotes how many observations are in that line.</p>

<p>Note that the cumulative count increases from the top as you move down the rows until the line containing the parentheses. Likewise, note that the cumulative count increases from the bottom as you move up the rows until the line with the parentheses in it.</p>

<p>You could count either way—either from the first observation or from the last observation—and arrive at the median. Importantly, remember to read the numbers from right to left if you start at the bottom. (You would read the numbers from left to right, of course, if you start at the top.)</p>

<p>In this case, we are told that our number of observations (<em>n</em>) is 71, so we can use the position formula to determine that the median is in position:</p>

\[\text{Position of median} = \frac{(71+1)}{2} = 36\]

<ul>
<li><strong><em>To find the median beginning at the top</em></strong>, go down to the line with a cumulative label of "32." This means that, at the end of this line, there have been a total of 32 observations. Then go down to the next line (the one labeled with parentheses), and go over four numbers (because 32 + <strong>4</strong> = 36). This is your median!</li>

<li><strong><em>To find the median beginning at the bottom</em></strong>, go up to the line with a cumulative label of "24." This means that, at the end of this line, there have been a total of 24 observations. Then go up to the next line (the one labeled with parentheses), and go over <strong>12</strong> numbers (because 24 + 12 = 36). (Be sure that you are reading from right to left!) This is your median!</li>
</ul>

<p>Notice that the leaf unit in this case is 0.1, which is different than the leaf unit of 1.0 that we used before. This indicates that, once we combine our stem and leaf, we have to multiply by 0.1 to get the final value of the observation. In this case, for example, the median has a stem of 3 and a leaf of 4, so this observation is 34 × 0.1 = <strong>3.4</strong>. The median semester GPA of a student in this economics course is therefore 3.4.</p>

<p>Let's see how the median would differ in this case if the leaf units were different:</p>
<ul>
<li><em>If the leaf unit were 1.0</em>, the median (3|4) would be 34 × 1.0 = <strong>34</strong>.</li>
<li><em>If the leaf unit were 10</em>, the median (3|4) would be 34 × 10 = <strong>340</strong>.</li>
<li><em>If the leaf unit were 0.01</em>, the median (3|4) would be 34 × 0.01 = <strong>0.34</strong>.</li>
</ul>


<div class="date">Wednesday, 9/3/25</div>

<p>In this section, we will discuss various ways to analyze the association between <strong>two</strong> variables.</p>
<ul>
<li>The <strong>explanatory variable</strong> (or <em>predictor variable</em>) is the independent variable—what we manipulate in an experiment, or what we use to explain the behavior of the response variable.</li>
<li>The <strong>response variable</strong> is the dependent variable. This is usually the variable we're trying to make a statement about.</li>
</ul>

<h1>Association Between Categorical Variables</h1>

<p>We use <strong>conditional proportions</strong> or <strong>percentages</strong> based on <strong>contingency tables</strong> to describe the association between categorical variables (i.e., when both the explanatory and response variables are categorical). Contingency tables typically describe frequencies (or counts). Based on this information, you can calculate conditional proportions (by dividing each cell count by the total number of observations in its <em>explanatory</em>-variable group) to determine the association between the two variables.</p>

<p>For example, consider the following table of survey results, which shows the number of males and females who tailgated before the Florida Gators home game against Long Island University.</p>

<table>
<thead>
<tr>
<th></th>
<th>Tailgated for > 2 Hours</th>
<th>Tailgated for < 2 Hours</th>
<th>Did Not Tailgate at All</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Males</td>
<td>20</td>
<td>42</td>
<td>12</td>
<td>74</td>
</tr>
<tr>
<td>Females</td>
<td>12</td>
<td>40</td>
<td>36</td>
<td>88</td>
</tr>
<tr>
<td>Total</td>
<td>32</td>
<td>82</td>
<td>48</td>
<td>162</td>
</tr>
</tbody>
</table>

<p>The survey is trying to answer the question: "Do more males tailgate than females?" In this case, <em>gender</em> is the explanatory variable, and <em>how much a person tailgated</em> is the response variable. Thus, the gender totals are our denominators because gender is the explanatory variable.</p>

<p>We could calculate three conditional proportions for each gender as follows:</p>

<table>
<thead>
<tr>
<th></th>
<th>Tailgated for > 2 Hours</th>
<th>Tailgated for < 2 Hours</th>
<th>Did Not Tailgate at All</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Males</td>
<td>\(\frac{20}{74} = 0.270\)</td>
<td>\(\frac{42}{74} = 0.568\)</td>
<td>\(\frac{12}{74} = 0.162\)</td>
<td>\(\frac{74}{74} = 1.000\)</td>
</tr>
<tr>
<td>Females</td>
<td>\(\frac{12}{88} = 0.136\)</td>
<td>\(\frac{40}{88} = 0.455\)</td>
<td>\(\frac{36}{88} = 0.409\)</td>
<td>\(\frac{88}{88} = 1.000\)</td>
</tr>
<tr>
<td>Total</td>
<td>\(\frac{32}{162} = 0.198\)</td>
<td>\(\frac{82}{162} = 0.506\)</td>
<td>\(\frac{48}{162} = 0.296\)</td>
<td>\(\frac{162}{162} = 1.000\)</td>
</tr>
</tbody>
</table>

<p>It would be fair to say that males in the sample tailgated more than females. Whereas only 16.2% of male respondents did not tailgate at all, 40.9% of female respondents did not tailgate at all. Note that what is important is not the <em>number</em> of males or females in the survey who tailgated; what's important is the <em>proportion</em> of males or females surveyed who tailgated.</p>

<div class="exam-tip">
<h4>EXAM TIP: Contingency Table Analysis</h4>

<p>If you're given a contingency table like this on your exam, read the question(s) carefully, and be sure to use the appropriate denominator(s).</p>
</div>

<h1>Association Between Quantitative Variables</h1>

<p>We use <strong>scatterplots</strong> to describe the association between two quantitative variables. Typically, the explanatory variable (the <em>independent variable</em>) is plotted on the horizontal <em>x</em>-axis, and the response variable (the <em>dependent variable</em>) is placed on the vertical <em>y</em>-axis.</p>

<p>For example, in the graphs below, Variable B is the explanatory variable, and Variable A is the response variable. Sometimes, you will definitely know which variable is the explanatory variable and which is the response variable (e.g., father's height and son's height). In other cases, you won't (e.g., father's height and mother's height). When it is not clear which variable is which, it is appropriate to put either variable on either axis. The correlation between two variables is the same no matter which variable is called the response variable and which variable is called the explanatory variable.</p>

<p>Note that the word "correlation" can ONLY be used to describe the relationship of <em>quantitative</em> variables. (For categorical variables, "association" or "relationship" can still be used.) The <strong>correlation coefficient (<em>r</em>)</strong> is a quantitative measure that tells us about the <em>strength</em> and <em>direction</em> of a linear relationship between two quantitative variables.</p>

<ul>
<li><b><em>Direction</b></em> – A <em>positive</em> correlation indicates a positive slope—that one variable tends to increase when the other increases. A <em>negative</em> correlation indicates a negative slope—that one variable tends to decrease when the other increases.</li>
<li><b><em>Strength</b></em> – The larger the magnitude of the correlation, the stronger the relationship.</li>
</ul>

<p>The correlation coefficient (<em>r</em>) can take on any value between −1.00 and +1.00. This measure has no units, and it is strongly affected by outliers.</p>

<ul>
<li><strong><em>r</em> = +1.00</strong> indicates a perfect positive correlation between the two variables.</li>
<li><strong><em>r</em> = −1.00</strong> indicates a perfect negative correlation between the two variables.</li>
<li><strong><em>r</em> = 0</strong> indicates that there is no linear correlation between the two variables.</li>
</ul>

<div class="correlation-grid">
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-01-correlation-perfect-positive.png" 
     alt="Scatterplot showing perfect positive correlation with r = +1.00" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-02-correlation-strong-positive.png" 
     alt="Scatterplot showing strong positive correlation with r = +0.85" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-03-correlation-no-linear.png" 
     alt="Scatterplot showing no linear correlation with r = 0 (scattered points)" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-04-correlation-perfect-negative.png" 
     alt="Scatterplot showing perfect negative correlation with r = -1.00" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-05-correlation-moderate-negative.png" 
     alt="Scatterplot showing moderate negative correlation with r = -0.5" 
     loading="lazy" 
     width="auto" 
     height="auto" />
<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-06-correlation-u-shaped.png" 
     alt="Scatterplot showing U-shaped pattern with r = 0 (non-linear relationship)" 
     loading="lazy" 
     width="auto" 
     height="auto" />
</div>

<p>The U-shaped scatterplot above has a correlation coefficient of zero, even though the data have a pretty distinct pattern, because the correlation coefficient only describes <em>linear</em> correlations; you should not use it if you're dealing with a non-linear relationship.</p>

<h1>Using Simple Linear Regression to Predict Outcomes</h1>

<p>In this section, we will discuss how we might use regression to predict the outcome of a variable, based on a set of known data points. A <strong>regression line</strong> summarizes the relationship between a set of data points. The general equation for this line is as follows:</p>

<p>\[\hat{y} = a + bx\]</p>

<p>In this equation, <em>a</em> is the vertical intercept (the <em>y</em>-intercept), <em>b</em> is the slope of the regression line, and \(\hat{y}\) is the predicted value (of the response variable). A sample regression line is shown below. Note that the predicted value at <em>x</em><sub>1</sub> is \(\hat{y}\).</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-07-regression-line-diagram.png" 
     alt="Regression line diagram showing explanatory variable x, response variable y, vertical intercept a, slope b, and predicted value at x1" 
     class="image-wide-50" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<div class="exam-tip">
<h4>BEWARE: Regression Equation Notation</h4>

<p>In algebra, you probably learned that the equation for a line is y = mx + b. In that formula, b was the vertical intercept, and m was the slope. Well, you can forget all that for regression! In the regression line equation, b stands for the SLOPE, and a is the vertical intercept. Don't get these confused!</p>
</div>

<p>Note the following about our regression line equation:</p>

<ul>
<li><b>The regression line reflects a tendency that holds true on average, so we can plug in a particular <em>x</em>-value to find an expected or predicted value of <em>y</em>.</b></li>
<li><b>The slope, <em>b</em>, represents the change in <em>y</em> that we would expect due to a one-unit change in <em>x</em>.</b></li>
<li><b>The vertical intercept, <em>a</em>, should not always be interpreted.</b> The vertical intercept is the value of <em>y</em> when <em>x</em> is zero. We only interpret this point if it makes sense for <em>x</em> to be zero, and if we have data close enough to zero that it would not distort our calculations. For example, if we have a graph comparing human height (on the horizontal axis) and human weight (on the vertical axis), we would not interpret the vertical intercept of the regression line because no human is zero inches tall.</li>
</ul>

<div class="example-box">
<h4>Example: Predicting the Outcome of a Variable</h4>

<p>The regression equation that predicts weight (in pounds) based on height (in inches) in male college students is given as follows:</p>

<p>\[\hat{y} = -220 + 6x\]</p>

<p>In this case, the slope (<em>b</em>) is 6, so we would expect a six-pound increase in body weight (<em>y</em>) with every one-inch increase in height (<em>x</em>). The <em>y</em>-intercept is −220, but we do not interpret this point because it has no practical meaning; <em>x</em> = 0 (a height of zero inches tall) is not realistic for human data.</p>

<p>We can plug height values into this equation to determine expected weight. For example, if height (<em>x</em>) = 60 inches, then predicted weight (\(\hat{y}\)) = −220 + 6(60) = <b>140</b> lbs. Therefore, the point (60, 140) is along the regression line.</p>
</div>

<p>When we have a number of data points on a scatterplot, we use the least-squares method to find a regression line that fits the points best. This method identifies the line that minimizes the <em>squares</em> of the <strong>residuals</strong>—the prediction errors. For each observation, the residual measures the vertical difference between the observed <em>y</em>-value and the predicted <em>y</em>-value.</p>

<p>\[\text{Residual} = y - \hat{y} = \text{Observed Value} - \text{Predicted Value}\]</p>

<div class="exam-tip">
<h4>EXAM TIP: Remembering Residuals</h4>

<p>Remember that the residual is the observed value minus the predicted value. Remember that o comes before p in the alphabet, and observed comes before predicted.</p>
</div>

<p>The <strong>least-squares regression method</strong> finds a "best fit" line for a series of points by minimizing the sum of the squared errors (i.e., setting the regression line where it has the least vertical deviation between the points and the line). Note the following about the least-squares regression method:</p>

<ul>
<li>The least-squares regression line minimizes the sum of the squared <em>residuals</em>, or <em>vertical distances</em>. Therefore, it is important to determine which is the explanatory variable <em>x</em>, and which is the response variable <em>y</em>.</li>
<li>The least-squares regression line passes through the middle of the points, such that the sum of the residuals is zero. That is, the sum of the <em>magnitude</em> of the residuals above the line is equal to the sum of the <em>magnitude</em> of the residuals below the line.</li>
<li>The least-squares regression line always passes through the point (\(\bar{x}\), \(\bar{y}\))—the point at the mean of both the <em>x</em>-values and the <em>y</em>-values.</li>
</ul>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-08-residuals-diagram.png" 
     alt="Diagram showing best fit regression line through data points with residuals marked as vertical lines from points to the line" 
     class="image-right-40" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>Running a regression using the least-squares method involves the following formulas (in addition to the general equation for the regression line):</p>

<p>\[b = r \left(\frac{s_y}{s_x}\right)\]</p>
<p>(This is the formula for the slope of the regression line.)</p>

<p>\[a = \bar{y} - b\bar{x}\]</p>
<p>(This is the formula for the <em>y</em>-intercept of the regression line.)</p>

<p>In these formulas…</p>
<ul>
<li><em>a</em> is the vertical intercept of the regression line.</li>
<li><em>b</em> is the slope of the regression line.</li>
<li><em>r</em> is the correlation coefficient.</li>
<li><em>s<sub>y</sub></em> is the standard deviation of the <em>y</em>-values.</li>
<li><em>s<sub>x</sub></em> is the standard deviation of the <em>x</em>-values.</li>
<li>\(\bar{x}\) is the average of the <em>x</em>-values.</li>
<li>\(\bar{y}\) is the average of the <em>y</em>-values.</li>
<li>\(\hat{y}\) is the predicted value.</li>
</ul>

<div class="exam-tip">
<h4>EXAM TIP: Order of Calculation</h4>

<p>Note that you have to find the slope of the regression line before you can find the vertical intercept, because the slope is a part of the formula for the vertical intercept. Stated another way, "We need <em>b</em> before we can find <em>a</em>."</p>
</div>

<h2>Coefficient of Determination (<em>R</em>²)</h2>

<p>The <strong>coefficient of determination</strong>, <strong><em>R</em>²</strong>, is simply equal to the square of the correlation coefficient, <em>r</em>. (Few people actually call it the "coefficient of determination." Most people just say, "R-squared.") The coefficient of determination tells us what percentage of the variability in the <em>y</em>-value can be explained by the linear regression of <em>x</em>.</p>

<div class="exam-tip">
<h4>EXAM TIP: Finding Correlation from <em>R</em>²</h4>

<p>You can find <em>r</em> by taking the square root of <em>R</em>² (in decimal form) and changing the sign to match the trend (i.e., be sure to make <em>r</em> negative if the trend is negative).</p>
</div>

<p>Suppose you are considering the relationship between height (on the <em>x</em>-axis) and weight (on the <em>y</em>-axis). Our regression analysis might tell us that a 6'-tall male should weigh about 162 pounds. We obviously wouldn't expect <em>all</em> 6'-tall males to weigh 162 pounds, because weight is affected by things other than height, such as food intake, exercise, and genetics. If we know that <em>R</em>² for this relationship is 60%, we could say that "60% of the variation in weight can be explained by height." In other words, 60% of the variability in weight can be explained by the regression on height. The other 40% of variation is due to other factors.</p>

<div class="date">Friday, 9/5/25</div><br>

<div class="example-box">

<h4>Example: Running a Regression</h4>

<p><b>The percentage of American households with computers has steadily increased over time, particularly during the internet boom. The following scatterplot shows 11 data points, corresponding to the percentage of households with computers from 1990 to 2000:</p>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter03-09-computer-ownership-scatterplot.png" 
     alt="Scatterplot titled 'Scatterplot of % HH with Computers vs. Time' showing an upward trend from 1990 to 2000, with y-axis from 55% to 100% and x-axis from 1990 to 2000" 
     class="image-wide-70" 
     loading="lazy" 
     width="auto" 
     height="auto" />

<p>Based on this information, we can answer the following questions:</p>

<p>a) Find the regression equation using the following information and assuming <em>r</em> = 0.9:</b></p>

<table>
<thead>
<tr>
<th></th>
<th>Year</th>
<th>% HH with Computers</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean</td>
<td>95</td>
<td>72.5</td>
</tr>
<tr>
<td>Standard Deviation</td>
<td>3.31</td>
<td>9.19</td>
</tr>
</tbody>
</table>

<p>At the outset, we should note a couple of things about the data we were given:</p>
<ul>
<li>Years are quantified with 2 numbers instead of 4, meaning "95" corresponds to 1995, "98" corresponds to 1998, and "100" corresponds to 2000.</li>
<li>The year is the variable on the <em>x</em>-axis and the percentage of households with computers is the variable on the <em>y</em>-axis. Therefore, we have been given \(\bar{x}\) = 95, \(\bar{y}\) = 72.5, <em>s<sub>x</sub></em> = 3.31, and <em>s<sub>y</sub></em> = 9.19 in this example.</li>
</ul>

<p>We can now use our regression equation formulas to find the slope, <em>b</em>, and <em>y</em>-intercept, <em>a</em>. We must find the slope first because the slope is needed to find the <em>y</em>-intercept:</p>

<p>\[b = r \left(\frac{s_y}{s_x}\right) = 0.9 \left(\frac{9.19}{3.31}\right) = 2.50\]</p>

<p>This slope says that the percentage of households with computers tends to increase by 2.5% per year. Another way to say this is that the percentage of households with computers increases by 2.5% per year, on average.</p>

<p>Now that we have calculated the slope, we can find the <em>y</em>-intercept:</p>

<p>\[a = \bar{y} - b\bar{x} = 72.5 - (2.50)(95) = -165\]</p>

<p>We do <strong>not</strong> interpret this <em>y</em>-intercept because <em>x</em> = 0 corresponds to year 1900, when computers had yet to be invented. Thus, although <em>x</em> = 0 is theoretically possible, it is too far from our data.</p>

<p>We can plug these values into our general formula for a regression line:</p>

<p>\[\hat{y} = a + bx\]</p>
<p>\[\hat{y} = -165 + 2.5x\]</p>

<p><b>b) What does our regression line predict will be the percentage of households with computers in 2002? What can you say about this prediction?</b></p>

<p>Remember that the year has been quantified with two digits instead of four. If "95" corresponds to 1995 and "100" corresponds to 2000, then <em>x</em> = 102 corresponds to 2002.</p>

<p>\[\hat{y} = -165 + 2.5(102) = 90\]</p>

<p>Therefore, our regression equation predicts that 90% of households had personal computers in 2002. There is no reason to think that this prediction cannot be trusted, because 2002 is close enough to our actual data.</p>

<p><b>c) Suppose that the actual percentage of households with a computer in 2002 was 88%. What is the residual for this point, and what does it mean?</b></p>

<p>The residual is the observed value (<em>y</em>) minus the predicted value (\(\hat{y}\)):</p>

<p>\[Residual = y - \hat{y} = 88 - 90 = -2 \text{ percentage points}\]</p>

<p>Because the residual is negative, we can tell that the observed value was below the predicted value. In this case, the actual percentage of households with personal computers (88%) was two percentage points lower than the predicted percentage of households with personal computers.</p>
</div>
<div class="date">Monday, 9/8/25</div>

<h1>Cautions in Analyzing Associations</h1>

<p>Three particular cautions in regression that we will consider in this section are as follows:</p>
<ul>
<li>There are limits on our ability to <em>extrapolate</em> trends using least-squares regression lines.</li>
<li>We should be aware of the effects of <em>influential outliers</em> on least-squares regression lines.</li>
<li>We should consider the impacts (such as Simpson's Paradox) of any potential <em>lurking variables</em>, and why correlation does not imply causation.</li>
</ul>

<h2>Extrapolation</h2>

<p>As we saw with the <em>y</em>-intercept in the previous example, least-squares regression lines are sometimes used to predict future values, which lie outside of the range of data. Using the regression equation for values of <em>x</em> that are not within the observed range is called <strong>extrapolation</strong>. If we extrapolate—by trying to use the least-squares regression line to predict <em>x</em>-values too far outside of the range—we may get an inaccurate prediction (such as a prediction over 100%).</p>

<h2>Impacts of Influential Outliers</h2>

<p>An <strong>influential outlier</strong> is an outlier whose <em>x</em>-value is so far outside the rest of the data that it causes a major change in the correlation coefficient, coefficient of determination, and least-squares regression line. In some cases, an influential outlier can change the sign of the slope, altering the direction of the trend identified by the least-squares regression line.</p>

<p>Here are some steps to take when your data has one or more outliers:</p>
<ol>
<li><em>Check to be sure that the outlier is not simply a typo.</em></li>
<li><em>Investigate.</em> Try to find out more about the outlier so you can determine whether the unusual observation really belongs in the data set.</li>
<li><em>If the point does not belong in the data set, you can ignore it.</em> However, if there is no reason to say that it does not belong in the data set, you can't just get rid of it.</li>
<li><em>If the point is valid, conduct regression analyses both with and without the outlier.</em> If the resulting relationships are similar, you can use either one. If the resulting relationships are very different, however, you should <em>collect more data</em> to determine the relationship between your two variables.</li>
</ol>

<h2>Impacts of Lurking Variables and Why Correlation Does Not Imply Causation</h2>

<p>Even an extremely strong association between two variables does not imply that a change in one variable <em>causes</em> another. For example, suppose researchers find a positive correlation between ice cream sales and shark attacks in a given area. This doesn't mean that ice cream causes shark attacks. It is more likely that there is a <strong>lurking variable</strong> impacting the relationship between <em>x</em> and <em>y</em>. Summer weather, for instance, could cause more people to purchase ice cream and cause more people to swim in the ocean (increasing their chances of being attacked by a shark).</p>

<p>On an exam, the question would have to tell you that multiple studies have been repeated to control for lurking variables before you could trust a claim like the ones we have considered above. <b>Smoking is the only exception</b>; you are expected to know that thousands of studies have been done to conclude that smoking causes cancer.</p>

<table>
<thead>
<tr>
<th>Caution</th>
<th>Key Idea</th>
<th>Example</th>
<th>Exam Tip</th>
</tr>
</thead>
<tbody>
<tr>
<td>Extrapolation</td>
<td>Predictions outside observed range may be unrealistic</td>
<td>Predicting >100% computer ownership for 2026</td>
<td>Only trust predictions within the data range</td>
</tr>
<tr>
<td>Influential outliers</td>
<td>Extreme <em>x</em>-values can distort slope, <em>r</em>, and R²</td>
<td>One very tall person with low IQ</td>
<td>Test regression with and without the outlier</td>
</tr>
<tr>
<td>Lurking Variables</td>
<td>Hidden variables can reverse or distort associations</td>
<td>Ice cream sales vs shark attacks (lurking = summer weather)</td>
<td>Correlation ≠ causation</td>
</tr>
</tbody>
</table>

<p>Lurking variables contribute to a statistical phenomenon known as Simpson's paradox. <strong>Simpson's paradox</strong> occurs when the association between two categorical variables is reversed when a third variable is added into the data. To illustrate Simpson's paradox, consider the following example: Suppose you ask women if they drink or not, and 25 years later you determine if they are alive or not. The data is below:</p>

<table>
<thead>
<tr>
<th></th>
<th>Dead</th>
<th>Alive</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Non-drinkers</td>
<td>77</td>
<td>273</td>
<td>350</td>
</tr>
<tr>
<td>Drinkers</td>
<td>61</td>
<td>289</td>
<td>350</td>
</tr>
</tbody>
</table>

<p>The proportions of drinkers and non-drinkers who died are as follows:</p>

<table>
<thead>
<tr>
<th></th>
<th>Mortality rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>Non-drinkers</td>
<td>\(\frac{77}{350} = 22\%\) dead</td>
</tr>
<tr>
<td>Drinkers</td>
<td>\(\frac{61}{350} = 17.4\%\) dead</td>
</tr>
</tbody>
</table>

<p>Judging from this data, you would probably conclude that drinking may actually be linked to <em>lower</em> mortality. But let's introduce a third variable—obesity status. The following table breaks up the same data based on whether or not the women were obese:</p>

<table>
  <thead>
    <tr>
      <th></th>
      <th colspan="2" style="text-align: center;">NOT OBESE</th>
      <th colspan="2" style="text-align: center;">OBESE</th>
    </tr>
    <tr>
      <th></th>
      <th style="text-align: center;">Dead</th>
      <th style="text-align: center;">Alive</th>
      <th style="text-align: center;">Dead</th>
      <th style="text-align: center;">Alive</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Non-drinkers</td>
      <td style="text-align: center;">6</td>
      <td style="text-align: center;">81</td>
      <td style="text-align: center;">71</td>
      <td style="text-align: center;">192</td>
    </tr>
    <tr>
      <td>Drinkers</td>
      <td style="text-align: center;">36</td>
      <td style="text-align: center;">234</td>
      <td style="text-align: center;">25</td>
      <td style="text-align: center;">55</td>
    </tr>
  </tbody>
</table>

<p>The proportions of deaths for both categories—obese and not obese—of drinkers and both categories of non-drinkers are as follows:</p>

<table>
<thead>
<tr>
<th></th>
<th colspan="2" style="text-align: center;">NOT OBESE</th>
<th colspan="2" style="text-align: center;">OBESE</th>
</tr>
<tr>
<th></th>
<th style="text-align: center;">Mortality Rate</th>
<th></th>
<th style="text-align: center;">Mortality Rate</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Non-drinkers</td>
<td style="text-align: center;">\(\frac{6}{87} = 6.9\%\) dead</td>
<td></td>
<td style="text-align: center;">\(\frac{71}{263} = 27\%\) dead</td>
<td></td>
</tr>
<tr>
<td>Drinkers</td>
<td style="text-align: center;">\(\frac{36}{270} = 13.33\%\) dead</td>
<td></td>
<td style="text-align: center;">\(\frac{25}{80} = 31.25\%\) dead</td>
<td></td>
</tr>
</tbody>
</table>

<p>It now appears that drinking is linked to <em>higher</em> mortality; for the group of obese women and for the group of non-obese women, non-drinkers tend to have a lower mortality rate.So, the paradox is that the association reversed: Drinking seems to be healthier when looking at the data as a whole, but not drinking seems to be healthier when breaking the data down by introducing the third variable of whether the women were obese. This is a <em>lurking variable</em>, and it has a stronger effect on the outcome (dead or alive) than whether the women drink.</p>

<p>We can use <strong>relative risk</strong> to describe the extent to which the proportion of drinkers who died exceeds the proportion of non-drinkers who died. Relative risk is calculated as a ratio of two sample proportions:</p>

<p>\[RR = \frac{\hat{p}_1}{\hat{p}_2}\]</p>

<p>In this case, we will define \(\hat{p}_1\) as the sample proportion of women who are drinkers and \(\hat{p}_2\) as the sample proportion of women who are non-drinkers. The relative risk for non-obese women and obese women can be calculated as follows:</p>

<p>\[RR_{\text{Not Obese}} = \frac{\hat{p}_{\text{Drinkers}}}{\hat{p}_{\text{Non-drinkers}}} = \frac{0.1333}{0.0690} = 1.93\]</p>

<p>\[RR_{\text{Obese}} = \frac{\hat{p}_{\text{Drinkers}}}{\hat{p}_{\text{Non-drinkers}}} = \frac{0.3125}{0.2700} = 1.16\]</p>

<p>For non-obese women, the sample proportion who died and were drinkers was 1.93 times the sample proportion who died and were non-drinkers. In other words, for a non-obese woman, the chance of dying almost doubles if the woman is a drinker. For obese women, the sample proportion who died and were drinkers was 1.16 times the sample proportion who died and were non-drinkers.</p>

<p>Note that we could have also simply found the difference in proportion of drinkers and non-drinkers who died. However, computing the ratio for relative risk is a better comparison for illustrating the effect of drinking on the women.</p>


<p class="date">Friday, 9/12/25</p>

<h1>Methods of Data Collection</h1>

<p>In general, data comes from either an experiment or an observational study:</p>

<ul>
<li>In an <strong>experiment</strong>, the researcher divides the subjects up and assigns them to experimental treatments. In experiments, the explanatory variable is manipulated in some way. Experiments are generally preferred to observational studies, but we are sometimes unable to use them due to ethical concerns.</li>

<li>In an <strong>observational study</strong>, researchers merely witness what's happening without assigning treatments to specific groups. For example, a <em><b>cross-sectional study</em></b> takes a snapshot in time to determine prevalence, a <em><b>prospective study</b></em> follows experimental units into the future to repeatedly gather data, and a <em><b>case-control study</em></b> compares people with a certain outcome (cases) to people without it (controls), looking backward to identify possible causes. The major problem with observational studies is that it is difficult to make conclusions about causation because lurking variables and explanatory variables might be <em>confounded</em>. Two variables are <strong>confounded</strong> when their effects on the response variable cannot be distinguished (i.e., they are so intertwined that we cannot tell which variable is actually causing the effect).</li>
</ul>

<h1>Strategies for Observational Studies Through Sampling</h1>

<p>One of the most important characteristics of a "good" sample is that it is not <strong>biased</strong> (i.e., that it doesn't systematically favor one outcome over another). The best way to avoid bias in surveys is by using a mechanical method to gather <strong>random samples</strong>, aka <strong>probability samples</strong>, which are representative of the entire population of interest. One type of random sample is a <strong>simple random sample (SRS)</strong>, in which every person in a population has equal chance of being selected because individuals are chosen by chance. Simple random samples are usually randomly selected by a computer from a numbered list of the <strong>sampling frame</strong>—all elements in the population.</p>

<p>Unlike random sampling, <strong>non-probability samples</strong> rely on easy and cheap methods of collecting data, such as social media or mobile apps, but most of this data is biased. Two types of non-probability samples that are causes for sample bias in surveys are as follows:</p>

<ul>
<li>A <strong>voluntary sample</strong> consists of people who volunteered to participate (i.e., <em>self-selection</em>), rather than people who were <em>randomly selected</em> to participate. Voluntary samples (e.g., call-in radio shows and internet polls) are biased; they consist of people who felt strongly enough about the issue to participate in the survey.</li>

<li>A <strong>convenience sample</strong> consists of people who are selected not at random but rather because selecting them is convenient or easy. Convenience samples are common on the news, such as when a reporter goes downtown, picks people, and asks them what they think about a particular issue. One way to overcome this bias is to survey at multiple places and at different times of the day.</li>
</ul>

<p>Some other potential sources of bias in a sample survey are as follows:</p>

<ul>
<li><strong>Undercoverage</strong> occurs when the sampling frame is missing certain parts of the population. For example, using the phonebook to randomly select a sample of Gainesville residents is undercoverage because not every resident has a landline.</li>

<li><strong>Nonresponse bias</strong> occurs when some people are unwilling to participate, and those people have different positions on relevant issues than those who participated. For example, busy parents with children may be less likely than old retirees to participate in a telephone survey because they have less free time.</li>

<li><strong>Response bias</strong> occurs when respondents give false information, either because of faulty memory or because they do not want to answer truthfully about sensitive topics (e.g., income, health, taboo topics).</li>

<li>The <b>wording</b> of a question could bias the survey results if the question is ambiguous or unclear, or if it prompts a person to respond in a particular way (i.e., leading questions).</li>
</ul>

<p>After selecting the sample, we can distribute the sample survey via personal interview (highest response rate and highest cost), telephone interview, or questionnaire (lowest response rate and lowest cost).</p>

<h2>Margin of Error for Sample Surveys</h2>

<p>When conducting a survey using random sampling and only two response options (such as "favorable/unfavorable"), the margin of error can be approximated using the following formula:</p>

\[
\text{Margin of error} \approx \frac{1}{\sqrt{n}}
\]
<p>Note that this formula is just an approximation, and it doesn't hold true in all cases. (We will discuss how to calculate the exact margin of error later in the semester.)</p>

<p>The margin of error accounts for the fact that a random sample may not be representative of the entire population. It does <em>not</em> account for any kinds of bias; it assumes the survey uses a non-biased, random sample.</p>

<p class="date">Monday, 9/15/25</p><br>

<div class="example-box">
<h4>Example: Margin of Error and Political Polls</h4>

<p><strong>Suppose you ask 2,000 randomly selected registered voters in Florida whether they intend to vote for the Democratic candidate in the 2028 election, and 49% of them say "yes." Can you be confident that the Democratic candidate will lose the majority in Florida?</strong></p>

<p>To answer this, we have to approximate the margin of error in a survey of 2,000 people:</p>

\[
\text{Margin of error} \approx \frac{1}{\sqrt{n}} = \frac{1}{\sqrt{2,000}} \approx 0.022
\]

<p>This means we would not be surprised if the actual percentage of voters who intend to vote for the Democrat is anywhere between 46.8% (49% - 2.2%) and 51.2% (49% + 2.2%). We can NOT be confident that Democrats will lose in Florida because this interval includes values of 50% or more.</p>

<p><strong>Suppose the poll indicated that only 46% of the sample intends to vote Democrat.</strong></p>

<p>In this case, you can be confident that the Democratic candidate would lose Florida's electoral votes; the range that falls within the margin of error is now 43.8% to 48.2%, so no values of 50% and above (the requirement to win) fall within this range.</p>

</div>

<h1>Strategies for Experimentation</h1>



<p>Recall that an <em>experiment</em> differs from an observational study in that treatments are applied to different groups in an experiment. A <strong>treatment</strong> (the <em>x</em>-variable) is an experimental condition given to <strong>experimental units</strong>—the individuals or subjects involved in the experiment. The end result is to observe the various treatments' effects on the <strong>response variable</strong> (the <em>y</em>-variable)—the variable we're interested in measuring and drawing conclusions about.</p>
<div class="exam-tip">
<h4>EXAM TIP: Experimental Units</h4>

<p>In most of the experiments we discuss, the experimental units are people, but experimental units can be people, animals, or even objects (e.g., machines).</p>
</div>
<p>Three characteristics of a "good" experiment include the following:</p>

<ul>
<li><strong>Control of variability</strong> – A "good" experiment takes steps to minimize the impact of variability due to lurking variables and confounding effects. Some techniques for controlling variability are as follows:

<ul>
<li><em>Minimizing differences between experimental units</em> – Researchers must take steps to ensure that all of the experimental units operate under the same environmental conditions. Ideally, environmental conditions should be rigidly controlled in a laboratory setting, but this is not possible in most cases.</li>

<li><em>Using <strong>comparative experiments</strong></em> – It is a good idea to compare one treatment to at least one other, rather than just observing the performance of the treatment you're interested in, to eliminate confounding effects.</li>

<li><em>Using control groups</em> – A <strong>control group</strong> is a group that gets either the placebo treatment or no treatment at all. The control group serves as a basis for comparing the performance of the treatment on the experimental units. Note that a control group is not necessary when comparing more than 1 treatment.</li>

<li><em>Using placebos</em> – Participants' expectations of a certain outcome may make that outcome more likely to occur. The <strong>placebo effect</strong> happens when a result occurs due to a participant's expectations rather than a change in the independent variable. Researchers control for variability due to the placebo effect by giving some participants <strong>placebos</strong>—dummy treatments with no active ingredients. For example, participants in a clinical drug trial might be given a sugar pill.</li>

<li><em>Using blind and double-blind studies</em> – To avoid subjects' expectations about whether the treatment will work impacting the treatment's effectiveness, researchers use blind studies. A <strong>blind study</strong> is one in which the subject does not know which treatment he or she is getting.

<p>Moreover, the person who is administering the treatment may inadvertently bias the study in some way if he or she knows which treatment the placebo is. Researchers can avoid this kind of bias and variability through the use of double-blind studies. A <strong>double-blind study</strong> is one in which neither the subjects nor the person who deals directly with the subjects, know which treatment is being given.</p></li>
</ul>
</li>

<li><strong>Randomization</strong> – A "good" experiment uses random sampling and random assignment of treatments to experimental units.

<ul>
<li><em>Random sampling</em> – As we have discussed, we can avoid the confounding effects of lurking variables by using a mechanical method to randomly pull from the population when constructing a sample. Random sampling also allows us to use the laws of probability to analyze the results.</li>

<li><em>Random assignment of treatments</em> – Researchers must also use a mechanical method to randomly assign treatments to individual units. If treatments are not randomly allocated to patients, the person doing the allocation of treatments might (intentionally or unintentionally) bias the sample (e.g., assign the placebo to the least healthy patients).</li>
</ul>
</li>

<li><strong>Replication</strong> – A "good" experiment applies each treatment to a large number of experimental units. That is, it has a large number of <strong>replications</strong>. The more replications you have, the less likely it is that a treatment that appears to be superior is only superior because of luck. Thus, more replications allow us to have more confidence in our results.</li>

<div class="exam-tip">
<h4>EXAM TIP: Replication vs Repeatability</h4>

<p>Don't confuse the statistical definition of replication with repeatability, which is whether an experiment can be repeated by other researchers. In this class, replications refer only to the number of experimental units per treatment.</p>
</div>
</ul>

<h2>Multifactor Experiments</h2>

<p>Unlike a <em>one-factor</em> study, a <strong>multifactor experiment</strong> involves two or more factors. <strong>Factors</strong> are an experiment's categorical explanatory variables. The different alternatives under each factor category are the factors' <strong>levels</strong>. The <strong>treatments</strong> are the combinations of factors and levels. The number of <strong>replications</strong> refers to the number of experimental units per treatment group. The best way to understand this terminology is with an example.</p>

<p>Suppose researchers wanted to assess the effectiveness of the various LSAT prep courses on math majors and English majors. They identify 12 math majors and 12 English majors. In each group, 3 people are assigned to the Kaplan course, 3 to Magoosh, 3 to Blueprint, and 3 to no course at all. Their LSAT scores are recorded after they get their results back.</p>

<ul>
<li><b>Response variable</b> – The response variable (the <em>y</em>-variable) is each student's LSAT score.</li>

<li><b>Experimental units</b> – There are 24 experimental units: 12 math majors and 12 English majors.</li>

<li><b>Factors</b> – This is a case in which there are 2 factors: major and LSAT course.</li>

<li><b>Levels</b> – The different levels for each factor are as follows:
<ul>
<li><em>Major</em> has 2 levels: math majors and English majors</li>
<li><em>LSAT course</em> has 4 levels: Kaplan, Magoosh, Blueprint, and no course</li>
</ul>
</li>

<li><b>Treatments</b> – The different treatments are combinations of each factor and each level. The best way to illustrate these combinations is to create simple fan diagrams:</li>

<img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter04-01-fandiagramlsattreatments.png" alt="Fan diagram showing treatment combinations for math and English majors with different LSAT courses" class="image-wide-95" loading="lazy" width="auto" height="auto" />

<p>So, there are 8 different treatments here:</p>
<ul>
<li>Math majors who took the Kaplan course</li>
<li>Math majors who took the Magoosh course</li>
<li>Math majors who took the Blueprint course</li>
<li>Math majors who took no LSAT course at all</li>
<li>English majors who took the Kaplan course</li>
<li>English majors who took the Magoosh course</li>
<li>English majors who took the Blueprint course</li>
<li>English majors who took no LSAT course at all</li>
</ul>

<div class="exam-tip">
<h4>EXAM TIP: Calculating Number of Treatments</h4>

<p>To determine how many treatments there are, multiply the number of levels in each factor. For this case, we would multiply the number of levels in the first factor (2 major levels) by the number of levels in the second factor (4 course levels) to get 8 different treatments. The number of factors only tells us how many numbers will be multiplied together.</p>
</div></ul>

<ul>
<li><b>Replications</b> – In this case, there are 3 replications because there were three experimental units in each of the treatment groups \((24 \text{ experimental units} \div 8 \text{ treatment types} = 3 \text{ replications})\).</li>
</ul>

<p class="date">Wednesday, 9/17/25</p>

<h2>Matched Pairs Design</h2>

<p>Another advanced form of experimentation is the <strong>matched pairs design</strong>, in which the same or similar experimental units are matched, and each receives a different treatment. Matching similar experimental units is useful because it controls for confounding variables. For example, matching twins together and administering different treatments is useful because twins share genetic and physiological commonalities, so we can control for what would otherwise be differences between our experimental units that might bias the results. Finding a large number of twins is difficult, so researchers often match individuals up with themselves, which is called a <em>cross-over design</em>.</p>

<h2>Ethical Research Experiments</h2>

<p>An <strong>Internal Review Board (IRB)</strong> ensures that the benefits of experiments exceed the risks to human subjects, and it ensures that existing risks are minimal. <em>Experiments that involve human subjects require supervision by an IRB</em>, which is formally designated by the institution conducting the research, such as a university or a hospital. Experiments under IRBs must follow certain ethical standards and guidelines, including requirements of <b>informed consent</b>:</p>

<ul>
<li>Researchers must disclose to participants the <b><em>details</b></em> regarding what the study is and how it will be conducted.</li>

<li>Potential <b><em>risks and benefits</b></em> must be fully explained by researchers and understood by participants.</li>

<li>Participation by human subjects must be <b><em>voluntary</b></em>; subjects cannot be coerced.</li>
</ul>

<p>The <strong>Belmont Report</strong>, which was written to address inhumane research on human subjects, summarizes three basic principles at the core of ethical research:</p>

<ul>
<li><strong>Respect for persons</strong> – Treat subjects with respect, explain options, and obtain informed consent.</li>

<li><strong>Beneficence</strong> – Do no harm and secure participants' well-being by ensuring a proper benefit-to-risk ratio.</li>

<li><strong>Justice</strong> – Appropriately select subjects so that specific portions of the population (such as prisoners, minorities, or the mentally ill) are not bearing an unequal share of the risk.</li>
</ul>


<p class="date">Friday, 9/19/25</p>

<h1>Using Probability to Quantify Randomness</h1>

<p>In statistics, a "random" phenomenon is more predictable than you might think. A phenomenon is <strong>random</strong> if you cannot predict the next outcome based on previous outcomes. However, many outcomes may allow you to detect a predictable pattern of long-run randomness. For example, if you flip a coin hundreds of thousands of times, you do not know whether any single flip will produce a heads or tails, but you do know that you would get heads about half of the time.</p>

<p>We use probability to help us better understand sampling distributions and make statements using statistical inference. The <strong>probability</strong> of a random outcome is the proportion of occurrences of that outcome in an extremely long series of independent trials. It is equal to the <strong>relative frequency</strong> of that outcome over the long run. We use P(x) to denote the probability of x event occurring. Probabilities range between 0 and 1, and the probabilities of all possible outcomes must sum to 1.</p>

<p>Trials are <strong>independent</strong> if one outcome is not affected by other outcomes. Things like rolling dice or flipping coins involve independent trials. For example, if you flip tails 10 times in a row, the probability of getting tails again on your 11th flip is still 50%; the fact that you have already flipped so many tails does not affect this probability.</p>

<p>To illustrate a case in which two events are <em>dependent</em> (i.e., not independent), you might consider the example of drawing from a deck of playing cards. A standard deck of playing cards consists of 52 cards, four of which are Queens. The probability of drawing a Queen on your first draw is therefore \(\frac{4}{52}\). If you were to draw a <em>second</em> card, assuming you didn't put the first card back in the deck, the probability of drawing a Queen would either be \(\frac{3}{51}\) (if you drew a Queen on the first card) or \(\frac{4}{51}\) (if you didn't). In this case, the probability of the outcome of the second draw depends on the outcome of the first draw, so these events are <em>dependent</em>.</p>

<h1>Finding Probabilities</h1>

<p>When finding probabilities, the <strong>sample space</strong> is the set of all possible outcomes. We use a cursive S and curly brackets to denote the sample space (shown below). In a description of the sample space, all of the potential outcomes are found within the curly brackets. An <strong>event</strong> is a particular outcome. It is a subset of the sample space. The probabilities of all events in the sample space must sum to one.</p>

<p>For example, suppose we tossed a fair coin and recorded the number of tails. There are only 2 possible events: 0 tails or 1 tail. The sample space and probabilities are as follows:</p>

<p>S = {0, 1}<br>
P(0) = 0.5<br>
P(1) = 0.5</p>



<p>The following are the basic rules of probability:</p>

<ul>
<li><em>Complement rule</em> – The probability that an event will <em>not</em> happen is equal to 1.00 minus the probability that the event <strong>will</strong> happen. We use a superscript c to denote the complement. You may find it helpful to read P(A<sup>c</sup>) as "the probability of NOT A," or "the probability that A does not occur."

<p>\[P(A^c) = 1 - P(A)\]</p>

<p>For example, when rolling a balanced die, the probability that we don't get a 5 (i.e. the probability that we get 1, 2, 3, 4, or 6) is equal to 1 minus the probability that we roll a 5.</p>

<p>\[P(5^c) = 1 - P(5) = 1 - \frac{1}{6} = \frac{5}{6}\]</p>
</li>

<li><em>Intersection</em> – The <strong>intersection</strong> of 2 events refers to the outcomes that are common to each event. We use the symbol ∩ to represent "the intersection of." So, P(A ∩ B) refers to the probability that an outcome is in the intersection of events A and B.

<p>\[P(A \cap B) = P(A \text{ and } B)\]</p>

<p>We can find the probability that we roll an odd number AND we roll a number smaller than 4 by finding the intersection of these two events. The outcomes that satisfy both of these conditions are 1 and 3.</p>

<p>\[P(\text{Odd and Less than 4}) = P(1, 3) = \frac{2}{6}\]</p>
</li>
<div class="exam-tip">
<h4>EXAM TIP: Formula Memorization</h4>

<p>Remember that formulas shown in boxes should be memorized, as they may not be provided on the exam, whereas the shaded portion of a formula will be on your Exam 1 formula sheet.</p>
</div>
<li><em>Union</em> – The <strong>union</strong> of two events consists of all events in either or both of those events. We can calculate the union by adding the probability of each event together and subtracting the intersection.

<p>\[P(A \cup B) = P(A \text{ or } B) = P(A) + P(B) - P(A \text{ and } B)\]</p>

<p>We can find the probability that we roll an even number OR roll a number smaller than 5 by adding the probability that we roll an even number (2, 4, or 6) and the probability that we roll a number smaller than 5 (1, 2, 3, 4), and then subtracting the intersection.</p>

<p>\[P(\text{Even or } < 5) = P(\text{Even}) + P(< 5) - P(\text{Even and } < 5) = \frac{3}{6} + \frac{4}{6} - \frac{2}{6} = \frac{5}{6}\]</p>

<p>Two events are called <strong>disjoint events</strong> when they do not share any outcomes in common. When this is the case, <strong> P(A ∩ B) = 0</strong>, so the probability that <em>either</em> occurs can be found using the union rule by simply adding their probabilities (and subtracting zero). We can find the probability that we roll an odd number OR we roll a 6 by adding the probability that we roll an odd number (1, 3, or 5) and the probability that we roll a 6.</p>

<p>\[P(\text{Odd or 6}) = P(1, 3, 5, 6) = P(\text{Odd}) + P(6) - 0 = \frac{3}{6} + \frac{1}{6} = \frac{4}{6}\]</p>
</li>

<li><em>Multiplication rule</em> – The <strong>multiplication rule</strong> states that the probability of <strong>both</strong> of two independent events occurring is simply the product of their probabilities:

<p>\[P(A \cap B) = P(A \text{ and } B) = P(A) \times P(B)\]</p>

<p>For example, suppose we roll a die twice. We can find the probability we roll a 3 on the first roll and a 3 on the second roll by multiplying the probability of each.</p>

<p>\[P(\text{Two 3s}) = P(3) \times P(3) = \frac{1}{6} \times \frac{1}{6} = \frac{1}{36}\]</p>
</li>
</ul>

<div class="exam-tip">
<h4>EXAM TIP: Rule Applications</h4>

<p>Notice that the complement, intersection, and union rules apply only to data from the same trial, whereas the multiplication rule applies to data from at least two sequential trials.</p>
</div>

 <p class="date">Monday, 9/22/25</p>
    
    <h1>Using Probability Distributions to Summarize Possible Outcomes</h1>
    
    <p>In this chapter, we will focus on probability distributions for a <strong>random variable</strong>, a numerical measurement of a random phenomenon's outcome. We commonly use capital letter X or Y to denote a random variable. Examples of random variables include exam scores, height, and the number of heads we get when we flip a coin 10 times. There are 2 kinds of random variables:</p>
    
    <ul>
        <li><strong>A discrete random variable</strong> has a finite list of possible outcomes. For example, the number of heads you get when you flip a coin 3 times has only 4 outcomes: 0, 1, 2, and 3.</li>
        <li><strong>A continuous random variable</strong> has an infinite list of possible outcomes. For example, the height of students in a classroom has an infinite list of possible outcomes because we could continue to divide units of measurement into smaller and smaller units. One person could be 60 inches, another person could be 60.1 inches, another person could be 60.11 inches, another could be 60.111 inches, etc.</li>
    </ul>
    
 <div class="exam-tip">
        <h4>EXAM TIP: Random Variable Definition</h4>
        <p>Note that a random variable must be numerical. For example, if we write down the gender of people sitting in a classroom, we are not working with a random variable because it is not numerical. (However, we could "cheat" and make this a random variable by writing 0 for every male and 1 for every female.)</p>
    </div>
    
    <p>When we consider probability distributions, we are considering entire populations, not just samples. Therefore, we are considering <strong>parameters</strong>—numerical summaries of populations, such as the <strong>population mean</strong> (μ) and the <strong>population standard deviation</strong> (σ).</p>
    
    <h2>Probability Distributions for Discrete Random Variables</h2>
    
    <p>A probability distribution for a discrete random variable specifies all of the possible outcomes and their corresponding probabilities. Because there is a finite number of possible outcomes for discrete random variables, each probability, P(X), must be between 0 and 1, and all of the probabilities must add to 1.</p>
    
    <p>We can summarize the probability distribution of a discrete random variable with a <strong>probability histogram</strong>. A probability histogram is similar to the histograms that we described in Chapter 2. The major difference is that we use lines instead of bars to denote the fact that we are dealing with discrete variables with finite values. For example, the number of heads you get when you flip a coin 3 times could only be 0, 1, 2, or 3. It couldn't be 2.5.</p>
    
    <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-01-probability-histogram-coin-flips.png" alt="Probability histogram showing distribution of heads in 3 coin flips with discrete values 0,1,2,3" class="image-right-25" loading="lazy" width="auto" height="auto" />
    
    <p>The mean of the probability distribution is equal to the sum of the products of the outcomes and the probabilities of those outcomes. That is,</p>
    
    <p>\[\mu = E(x) = \sum xP(x)\]</p>
    
    <p>The mean for a probability distribution is also called the <strong>expected value</strong>, or E(x). For example, if the mean of a probability distribution for the number of free throw shots made out of 10 attempts is 4 shots, we can say that the expected number of shots that the basketball player will make is 4 out of 10.</p>
    
       <h2>Probability Distributions for Continuous Random Variables</h2>
    
    <p>In this section, we will focus on probability distributions to summarize the possible outcomes for <strong>continuous random variables</strong>—random variables with an infinite number of possible outcomes. Whereas the probabilities of the various outcomes for a discrete variable are illustrated with lines at particular points along a histogram, probabilities of the various outcomes for a continuous variable are illustrated as <em>areas under a <b>smooth curve</b></em> (called a density curve). In fact, because there are an infinite number of possibilities for a continuous random variable, the probability of any given outcome occurring is zero (i.e., P(X = any specific value) = 0). Thus, these probabilities must be expressed over an <strong>interval</strong>.</p>
    
    <p>For the majority of the rest of the course, we will be working primarily with bell-shaped probability distributions. However, recall that distributions can take on a number of different shapes. Consider a <strong>random-number generator</strong>, which randomly picks a number between zero and one. The distribution of possible outcomes is "uniform," or "rectangular," and 10% of the area under the curve falls in each of the 10 intervals (0 to 0.1, 0.1 to 0.2, 0.2 to 0.3, etc.):</p>
    
    <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-02-uniform-distribution-rectangular.png" alt="Uniform distribution rectangular curve showing equal probability intervals from 0 to 1.0" class="image-wide-60" loading="lazy" width="auto" height="auto" />
     
    <p>Remember that when we're dealing with continuous random variables, probabilities are only assigned over intervals, so the probability that X is equal to any specific value is zero. For example, the probability that X is equal to exactly 0.5 is zero: P(X = 0.5) = 0.</p>

    <p>We could use this distribution to determine the probability that a random number falls in any given interval. For example, the shaded boxes below reveal that 60% of the area of the curve falls within the interval of 0.3 and 0.9. Therefore, the probability that X is between 0.3 and 0.9 is 60% or 0.6:</p>
    
    <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-03-uniform-distribution-shaded.png" alt="Uniform distribution with shaded region from 0.3 to 0.9 showing P(0.3 < X < 0.9) = 0.6" class="image-wide-60" loading="lazy" width="auto" height="auto" />
    
     <div class="exam-tip">
        <h4>EXAM TIP: Continuous Random Variables</h4>
        <p>Remember that the probability that x is equal to any particular value is zero because the normal distribution describes a continuous random variable.</p>
    </div>

    <h1>Finding Probabilities for Bell-Shaped Distributions</h1>
    
    <p>In this section, we will describe how to work with probabilities for bell-shaped distributions. A <strong>normal distribution</strong> is one that is bell-shaped and symmetric, with mean μ and standard deviation σ. The mean (μ) can be any number, but the standard deviation (σ) must be positive. The standard notation describing a random variable with a normal curve is as follows:</p>
    
    <p>\[X \sim N(\mu, \sigma)\]</p>
    
    <p>This is read as follows: "Random variable X is distributed as a normal distribution with a mean of μ and a standard deviation of σ."</p>

    <div class="exam-tip">
        <h4>EXAM TIP: Standard Deviation Requirements</h4>
        <p>Recall that the only way to have a standard deviation of zero is if all observations are the same; if this were the case, we wouldn't be dealing with a bell-shaped curve. Thus, standard deviation must be positive when you have a normal distribution.</p>
    </div>    
    
    <p>Note that very few, if any, random variables are perfectly normally distributed. One reason is that an exactly normal distribution has infinite range, meaning x has a minimum of negative infinity and a maximum of infinity.</p>
        
    <div class="exam-tip">
        <h4>EXAM TIP: Normal Distribution Recognition</h4>
        <p>Remember that not every distribution is a normal distribution. For example, if the scores on a test have a mean of 70 and a standard deviation of 30, you should recognize that you are not dealing with a normal curve! (We know this because of the empirical rule, which says that 99.7% of the distribution is within ±3 standard deviations of the mean. Three standard deviations below 70 would be −20, and a negative exam score is not possible. This indicates that this distribution is skewed right.)</p>
    </div>
    
    <h2>Standardizing Observations Using z-Scores</h2>
    
    <p>If we know the mean (μ) and standard deviation (σ) of a normally distributed random variable, we can draw a bell-shaped curve and shade the area under the curve relating to the interval we're interested in. However, there is no easy way to calculate the area under such a curve (any normal distribution), so we must <strong>standardize</strong> the given normal curve to a curve where z is distributed as a normal distribution with a mean of zero and a standard deviation of one unit. For the <strong>standard normal distribution</strong>, the notation is Z ~ N(0, 1).</p>
    
    <p>Luckily, <em>any</em> normally distributed distribution can be transformed into this standardized normal distribution by converting observations to <strong>z-scores</strong>:</p>
    
    <p>\[z = \frac{x - \mu}{\sigma}\]</p>
    
    <p>The z-score indicates how many standard deviations below or above the mean the observation is. A z-score is negative when the observation lies below the mean and positive when the observation lies above the mean. A z-score of 1.45, for instance, indicates that the observation lies 1.45 standard deviations above the mean. A z-score of −0.54 indicates that the observation is 0.54 standard deviations below the mean.</p>
    
    <p>The <strong>standard normal table</strong> (sometimes called the z-table) will be provided on your exam. We have also provided a standard normal table as a separate insert with your set of Smokin'Notes. You should be familiar with how to work with the standard normal table for the exam, as you will probably get multiple questions on it. It is important to understand that the standard normal table shows <strong>cumulative probabilities</strong>—probabilities to the left of the z-score. In other words, the numbers in the middle of the table give the area under the curve to left of a particular z-score (shown on the left and top edges of the table).</p>
    
    <p>There are 2 general types of questions that require this table, each with 3 variants.</p>
    
    <ul>
        <li><strong>Questions that ask for the probability</strong> – These questions usually give you a particular value for x and ask for the probability that an observation will be <em>less than</em> or <em>greater than</em> that. Sometimes, they give two values and ask for the probability that a random observation will be <em>between</em> those 2 values. Examples:
            <ul>
                <li>What is the probability that a randomly selected soda will have less than 11.9 oz?</li>
                <li>What is the probability that a randomly selected individual will have a score greater than 80?</li>
                <li>What percentage of batteries last between 20 and 24 hours?</li>
            </ul>
        </li>
    
    <p>When you're faced with such a question, you should follow these steps:</p>
    
    <ol>
        <li>Turn the given x value(s) into a z-score(s) using the formula.</li>
        <li>Draw the normal curve. Label the mean and the z-score(s) of interest. Shade the relevant area corresponding to the probability you're looking for.</li>
        <li>Use the standard normal table to determine the relevant area. Look up the digits of your z-score (at the edges of the table), and find the associated cumulative probability (where they intersect in the middle of the table).</li>
    </ol>
    
    <ul><ul>
        <li>If you shaded the area to the left, this cumulative probability is your answer.</li>
        <li>If you shaded the area to the right, subtract your cumulative probability from 1 to get your answer.</li>
        <li>If you shaded a middle area, subtract the smaller cumulative probability from the larger cumulative probability to get your answer.</li>
    </ul></ul></ul>
    
    <ul>
        <li><strong>Questions that ask for the value of x</strong> – These questions will give you a particular proportion and ask for the associated value(s) of x. Examples:
            <ul>
                <li>The slowest 10% of cars travel at speeds lower than _____.</li>
                <li>The top 1% of lightbulbs last _____ hours or more.</li>
                <li>The central 90% of phones last between _____ and _____ hours.</li>
            </ul>
        </li>
  
    
    <p>When you're faced with such a question, you should follow these steps:</p>
    
    <ol>
        <li>Draw the standard normal curve, label the mean, and shade in the area corresponding to the probability you're interested in.</li>
              
    <div class="exam-tip">
        <h4>EXAM TIP: Drawing Pictures</h4>
        <p>When you're trying to find the value of x, you absolutely must draw a picture. The numbers will get very confusing unless you can visualize a picture of the areas you are looking for.</p>
    </div>

        <li>Determine the appropriate probability, and look for it—or something close to it—in the middle of the standard normal table. Find the associated z-score (on the edges of the table).
            <ul>
                <li>If you shaded the area to the left, the given probability is the cumulative probability you will look for in the table.</li>
                <li>If you shaded the area to the right, subtract the given probability from 1 to find the cumulative probability you will look for in the table.</li>
                <li>If you shaded the middle area, you need to find two probabilities.
                    <ol>
                        <li>The first one is the proportion of the curve to the left of the shaded area (for example, if you are finding values for the central 90%, the area to the left of the shaded area is 5%, and you would look for 0.05 in the table).</li>
                        <li>The second one is this left-tail area plus the shaded area (for example, if you are finding values for the central 90%, the second probability is 95%, and you would look for 0.95 in the table).</li>
                    </ol>
                </li>
                    <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-04-normal-curve-central-90-percent.png" alt="Normal distribution curve showing central 90% area with 5% in each tail" class="image-wide-30" loading="lazy" width="auto" height="auto" />
            </ul>
        </li>
        <li>Use the formula to turn the z-score(s) you just found into a value of x to get your answer.</li>
    </ol>
    </ul>
    
    <p>To illustrate each type of question, we will use the following example:</p>
    
    <p><b><u><strong>Suppose that IQ is normally distributed with a mean of 105 and a standard deviation of 15.</strong></u></b></p>
        
    <h2>Probability That an Observation Is Less Than a Value</h2>
    
    <p><strong>What is the probability that a randomly selected individual from the population will have an IQ less than 125?</strong></p>
    
    <ol>
        <li><em>Turn the given x-value into a z-score using the formula.</em> In this case, x = 125:
            <p>\[z = \frac{x - \mu}{\sigma} = \frac{125 - 105}{15} = \frac{20}{15} = 1.33\]</p>
        </li>
        <li><em>Draw the normal curve. Label the mean and the z-score of interest. Shade the relevant area corresponding to the probability you're looking for.</em> A z-score of 1.33 means that this IQ is 1.33 standard deviations above the mean. Therefore, we will draw a boundary to the right of the mean. We are asked for the probability that a person will have an IQ less than 125, so we shade the area to the left.</li>
        
        <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-06-normal-curve-z-133-left-shaded.png" alt="Normal curve showing shaded area to the left of z = 1.33, representing 90.82% probability" class="image-wide-40" loading="lazy" width="auto" height="auto" />

        <li><em>Use the standard normal table to determine the relevant area. Look up the digits of your z-score (at the edges of the table), and find the associated cumulative probability (where they intersect in the middle of the table).</em> To find the cumulative probability associated with z = 1.33, start by looking for "1.3" in the leftmost column. Once you have found "1.3," move to the right until you reach the column with "0.03": <br><br>
        
        <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-05-standard-normal-table-excerpt.png" alt="Standard normal table excerpt showing how to find z-score of 1.33" class="image-wide-50" loading="lazy" width="auto" height="auto" />
        
        <p>The leftmost column tells you the first two digits of the z-score, and the top row tells you the last digit of the z-score.</p>
        
            <ul>
                <li><em>If you shaded the area to the left, this cumulative probability is your answer.</em> As you can see, the cumulative probability associated with a z-score of 1.33 is 0.9082, or 90.82%. This means that we can expect <strong>90.82%</strong> of the population to have an IQ <em>below</em> 125.</li>
            </ul>
        </li>
    </ol>
    
    <h2>Probability That an Observation Is Greater Than a Value</h2>
    
    <p><strong>What is the probability that a randomly selected individual from the population will have an IQ greater than 80?</strong></p>
    
    <ol>
        <li><em>Turn the given x-value into a z-score using the formula.</em> In this case, x = 80:
            <p>\[z = \frac{x - \mu}{\sigma} = \frac{80 - 105}{15} = \frac{-25}{15} = -1.67\]</p>
        </li>
        <li><em>Draw the normal curve. Label the mean and the z-score of interest. Shade the relevant area corresponding to the probability you're looking for.</em> A z-score of −1.67 means that this IQ is 1.67 standard deviations below the mean. Therefore, we will draw a boundary to the left of the mean. We are asked for the probability that a person will have an IQ greater than 80, so we shade the area to the right.</li>
            
 <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-07-normal-curve-z-negative-167-right-shaded.png" alt="Normal curve showing z = -1.67 with shaded area to the right representing 95.25% probability" class="image-wide-40" loading="lazy" width="auto" height="auto" />





        

        <li><em>Use the standard normal table to determine the relevant area. Look up the digits of your z-score (at the edges of the table), and find the associated cumulative probability (where they intersect in the middle of the table).</em> To find the cumulative probability associated with z = −1.67, start by looking for "−1.6" in the leftmost column. Once you have found "−1.6," move to the right until you reach the column with "0.07".
            <ul>
                <li><em>If you shaded the area to the right, subtract your cumulative probability from 1 to get your answer.</em> As you can see, the cumulative probability associated with a z-score of −1.67 is 0.0475 (which means that we can expect 4.75% of the population to have an IQ below 80). We know that the curve covers 100% of the population, so we find the proportion above 80 by subtracting from 1: 1 − 0.0475 = 0.9525, or 95.25%. This means that we can expect <strong>95.25%</strong> of the population to have an IQ above 80.</li>
            </ul>
        </li>
    </ol>
    
      
    <h2>Probability That an Observation Is Between Two Values</h2>
    
    <p><strong>What is the probability that a randomly selected individual from the population will have an IQ between 80 and 125?</strong></p>
    
    <ol>
        <li><em>Turn the given x-values into z-scores using the formula.</em> In this case, x = 80 and x = 125:
            <p>\[z = \frac{x - \mu}{\sigma} = \frac{80 - 105}{15} = \frac{-25}{15} = -1.67\]</p>
            <p>\[z = \frac{x - \mu}{\sigma} = \frac{125 - 105}{15} = \frac{20}{15} = 1.33\]</p>
        </li>
        <li><em>Draw the normal curve. Label the mean and the z-score(s) of interest. Shade the relevant area corresponding to the probability you're looking for.</em> A z-score of −1.67 means that this IQ is 1.67 standard deviations below the mean, so we draw this boundary to the left of the mean. A z-score of 1.33 means that this IQ is 1.33 standard deviations above the mean, so we draw this boundary to the right of the mean. We are asked for the probability that a person will have an IQ between these two values, so we shade the middle area.</li>

            
    <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-08-normal-curve-between-values-shaded.png" alt="Normal curve showing middle shaded area between z = -1.67 and z = 1.33, representing 86.07% probability" class="image-wide-40" loading="lazy" width="auto" height="auto" />

        <li><em>Use the standard normal table to determine the relevant area. Look up the digits of your z-score (at the edges of the table), and find the associated cumulative probability (where they intersect in the middle of the table).</em>
            <ul>
                <li><em>If you shaded a middle area, subtract the smaller cumulative probability from the larger cumulative probability to get your answer.</em> As you can see, the cumulative probabilities associated with z-scores of −1.67 and 1.33 are 0.0475 and 0.9082. We can find the proportion between them by calculating the difference: 0.9082 − 0.0475 = 0.8607. Therefore, <strong>86.07%</strong> of the population has an IQ between 80 and 125.</li>
            </ul>
        </li>
    </ol>
    
    <h2>Finding a Value of x Given a Proportion Less Than x</h2>
    
    <p><strong>What IQ corresponds with the bottom 5% of the population? (In other words: The bottom 5% of people have an IQ below ____.)</strong></p>
    
    <ol>
        <li><em>Draw the standard normal curve, label the mean, and shade in the area corresponding to the probability you're interested in.</em> In this case, we shade the bottom 5%.</li>
           
    <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-09-normal-curve-bottom-5-percent.png" alt="Normal curve showing bottom 5% shaded area with z = -1.645" class="image-wide-40" loading="lazy" width="auto" height="auto" />
    
        <li><em>Determine the appropriate probability, look for it—or something close to it—in the middle of the standard normal table. Find the associated z-score (on the edges of the table).</em>
            <ul>
                <li><em>If you shaded the area to the left, the given probability is the cumulative probability you will look for in the table.</em> Here, we shaded the bottom 5%, so we will look for 0.05 in the middle of the table. The closest possibilities listed are 0.0505 (associated with z = −1.64) and 0.0495 (associated with z = −1.65). When the probability we're looking for is halfway between two values, we use the average of those values (otherwise, we would pick whichever is closer). In this case, 0.05 is halfway between 0.0505 and 0.0495, so we will use z = −1.645.</li>
            </ul>
        </li>
        <li><em>Use the formula to turn the z-score you just found into a value of x to get your answer.</em>
            <p>\[z = \frac{x - \mu}{\sigma}\]</p>
            <p>\[-1.645 = \frac{x - 105}{15}\]</p>
            <p>\[x = -1.645(15) + 105\]</p>
            <p>\[x = 80.325\]</p>
        </li>
 
    
    <p>So, we can conclude that the bottom 5% of the population has an IQ less than <strong>80.325</strong>.</p>
    </ol>
    <h2>Finding a Value of x Given a Proportion Greater Than x</h2>
    
    <p><strong>What IQ corresponds with the top 40% of the population? (In other words: The smartest 40% of people have an IQ of at least ____.)</strong></p>

    <ol>
        <li><em>Draw the standard normal curve, label the mean, and shade in the area corresponding to the probability you're interested in.</em> In this case, we shade the top 40%.</li>

            <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-10-normal-curve-top-40-percent.png" alt="Normal curve showing top 40% shaded area with z = 0.25" class="image-wide-40" loading="lazy" width="auto" height="auto" />

        <li><em>Determine the appropriate probability, look for it—or something close to it—in the middle of the standard normal table. Find the associated z-score (on the edges of the table).</em>
            <ul>
                <li><em>If you shaded the area to the right, subtract the given probability from 1 to find the cumulative probability you will look for in the table.</em> Here, we shaded the top 40%, so we will look for 1 − 0.40 = 0.60 in the table. The closest possibility is 0.5987 (associated with z = 0.25).</li>
            </ul>
        </li>
        <li><em>Use the formula to turn the z-score you just found into a value of x to get your answer.</em>
            <p>\[z = \frac{x - \mu}{\sigma}\]</p>
            <p>\[0.25 = \frac{x - 105}{15}\]</p>
            <p>\[x = 0.25(15) + 105\]</p>
            <p>\[x = 108.75\]</p>
        </li>
   
        <p>This means that the bottom 60% of the population has an IQ less than 108.75. Therefore, we can conclude that the top 40% of the population has an IQ greater than <strong>108.75</strong>.</p>
     </ol>


    <h2>Finding Two Values of x Given a Proportion Between Them</h2>
    
    <p><strong>Between what two values will you find the central 80% of IQs? (In other words: The middle 80% of people have an IQ between _____ and _____.)</strong></p>
    
    <ol>
        <li><em>Draw the standard normal curve, label the mean, and shade in the area corresponding to the probability you're interested in.</em> In this case, we shade the middle 80%.</li>

    <img src="https://smokinnotes.s3.us-east-1.amazonaws.com/public/sta2023-253-exam1-chapter06-11-normal-curve-central-80-percent.png" alt="Normal curve showing central 80% with boundaries at z = -1.28 and z = 1.28" class="image-wide-40" loading="lazy" width="auto" height="auto" />

        <li><em>Determine the appropriate probability, look for it—or something close—in the middle of the standard normal table. Find the associated z-score (on the edges of the table).</em>
            <ul>
                <li><em>If you shaded the middle area, you need to find two probabilities:</em>
                    <ul>
                    <li>The first one is the proportion of the curve to the left of the shaded area. Since we shaded the central 80%, the area to the left is 10%, so we look for 0.10 in the middle of the table. The closest possibility is 0.1003 (associated with z = −1.28).</li>
                    <li>The second one is this left-tail area plus the shaded area. We shaded 80%, and the area to the left is 10%, so our second probability is 90%, and we look for 0.90 in the middle of the table. The closest possibility is 0.8997 (associated with z = 1.28).</li>
                </li></ul>
            </ul>
        </li>
        <li><em>Use the formula to turn the z-scores you just found into values of x to get your answers.</em>
            <p>\[z = \frac{x - \mu}{\sigma}\]</p>
            <p>\[-1.28 = \frac{x - 105}{15}\]</p>
            <p>\[x = -1.28(15) + 105\]</p>
            <p>\[x = 85.8\]</p>
            
            <p>\[z = \frac{x - \mu}{\sigma}\]</p>
            <p>\[1.28 = \frac{x - 105}{15}\]</p>
            <p>\[x = 1.28(15) + 105\]</p>
            <p>\[x = 124.2\]</p>
        </li>

        <p>This means that the bottom 10% of the population has an IQ less than 85.8, and the bottom 90% has an IQ less than 124.2 (which means the top 10% has an IQ above 124.2). Therefore, we can conclude that the central 80% of the population has an IQ <strong>between 85.8 and 124.2</strong>.</p>
        </ol>
    
    <p class="date">Wednesday, 9/24/25</p>
    
    <h1>Binomial Distribution</h1>
    
    <p>The <strong>binomial distribution</strong> displays probabilities for counts with binary (either-or) data. Three major differences between normal distributions and binomial distributions are as follows:</p>
    
    <ul>
        <li>The normal distribution is <em>continuous</em>, but the binomial distribution is <em>discrete</em>.</li>
        <li>The normal distribution is <em>always symmetric</em>, but the binomial distribution can be symmetric, skewed left, or skewed right.</li>
        <li>The parameters that describe a normal distribution are <em>mean (μ) and standard deviation (σ)</em>, whereas the parameters that describe a binomial distribution are the number of trials (n) and the probability of a success (p).</li>
    </ul>
    
    <p>A binomial distribution has four key characteristics:</p>
    
    <ul>
        <li>There must be a <strong>fixed number of trials</strong> (n).</li>
        <li>All n trials must be <strong>independent</strong>.</li>
        <li>Each trial must have the <strong>same probability of success</strong>.</li>
        <li>There must be a <strong>binary outcome</strong>. That is, you must be able to classify the outcome of a trial as a <em>success</em> or a <em>failure</em>.</li>
    </ul>
       <div class="exam-tip">
        <h4>EXAM TIP: BINS Memory Aid</h4>
        <p>The memory aid BINS may help you to remember these:</p>
        <ul>
            <li><b>B</b>inary outcome</li>
            <li><b>I</b>ndependent trials</li>
            <li><b>N</b>umber of trials is fixed</li>
            <li><b>S</b>ame chance of success</li>
        </ul>
    </div>
    <p>If all four of these conditions are met, the distribution of X—the number of successes—is a <strong>binomial distribution</strong> with parameters n (the number of trials) and p (the probability of a success). The standard notation is written in the following format (where the word "Binomial" is sometimes shortened to "Bin"):</p>
    
    <p>\[X \sim \text{Binomial}(n, p)\]</p>
    
    <p>In the context of a binomial distribution, a "success" is not necessarily a <em>good</em> thing. For example, we could be counting the number of cancer patients that died in a clinical trial of a new cancer treatment. In this case, a "success" would occur when a patient dies.</p>
    
    
    <div class="exam-tip">
        <h4>EXAM TIP: Identifying Binomial Problems</h4>
        <p>When you're looking at a problem on an exam, see if you can identify n, p, and X. If you can do so (and if the trials are independent), you're probably dealing with a binomial distribution problem.</p>
    </div>
    
    <h2>Probability Formulas for Binomial Distributions</h2>
    
    <p>Once we know we are working with a binomial distribution, we can use the following formulas:</p>
    
    <ul>
        <li><strong>Probability of x successes</strong> – The probability of getting exactly x successes in n trials is calculated as follows:</li>
    
    <p>\[P(x) = \binom{n}{x} p^x (1-p)^{n-x}\]</p>
    
    <p>Note that \(\binom{n}{x}\) in this equation is read, "n choose x," which means the same thing as, "Out of n trials, how many ways can we have exactly x successes?" which is as follows:</p>
    
    <p>\[\binom{n}{x} = \frac{n!}{x!(n-x)!}\]</p>
    
    <p>Note that n! is read as "n-factorial," and it is calculated as follows:</p>
    
    <p>\[n! = n \times (n - 1) \times (n - 2) \times (n - 3) \times \ldots \times 3 \times 2 \times 1\]</p>
   
    <div class="exam-tip">
        <h4>EXAM TIP: Factorial Calculations</h4>
        <p>For example, four factorial is calculated as follows:</p>
        <p>\[4! = 4 \times 3 \times 2 \times 1 = 24\]</p>
        <p>However, most scientific calculators have the capability to compute factorials for you.</p>
    </div>
    
    <p>Additionally, it's important to know that factorials for one and zero are both defined as equal to one:</p>
    
    <p>\[1! = 1 \text{ and } 0! = 1\]</p>
    
        <p>Putting this into the first equation, <strong>the probability of getting exactly x successes in n trials</strong> can be calculated as follows:</p>
    
    <p>\[P(x) = \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}\]</p>
    
        <li><strong>Mean</strong> – The mean of a binomial distribution is calculated as follows:</li>
    
    <p>\[\mu = np\]</p>
    <ul>
    <p>n = the number of trials<br>
    p = the probability of a success</p>
    </ul>
        <li><strong>Standard deviation</strong> – The standard deviation of a binomial distribution is calculated as follows:</li>
    </ul>
    
    <p>\[\sigma = \sqrt{np(1-p)}\]</p>
    
     <div class="example-box">
        <h4>Example: Binomial Distribution</h4>
        
        <p><strong>CoolBreeze laser treatments cure toenail fungus in 80% of patients. A doctor is treating 10 toenail fungus patients this week. Assume that the patients are unrelated and can be considered a random sample. Let X equal the number of patients who are cured.</strong></p>
        
        <p><strong>a) Is this a binomial distribution?</strong></p>
        <p>Yes. A success occurs when the treatment cures toenail fungus, and there are a fixed number of trials (n = 10), all of which are independent. The probability of a success is given (p = 0.80).</p>
        
        <p><strong>b) What is the sample space for this distribution?</strong></p>
        <p>The sample space is S = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.</p>
        
        <p><strong>c) How many of the 10 patients would you expect to be cured?</strong></p>
        <p>We can use our formula for the mean to answer this question:</p>
        <p>\[\mu = np = (10)(0.8) = 8\]</p>
        <p>It makes sense that we would expect <strong>8</strong> out of the 10 patients to be cured, because we know that the treatment is 80% effective.</p>
        
        <p><strong>d) What is the standard deviation of the distribution?</strong></p>
        <p>We can use our formula for the standard deviation to answer this question:</p>
        <p>\[\sigma = \sqrt{np(1-p)} = \sqrt{(10)(0.8)(0.2)} = \sqrt{1.6} = 1.265\]</p>
        
        <p><strong>e) What is the probability that the treatment cures toenail fungus in exactly 6 of the 10 patients?</strong></p>
        <p>We can use our probability formula to answer this question:</p>
        <p>\[P(6) = \binom{10}{6} (0.8)^6 (1-0.8)^{10-6} = \frac{10!}{6!(10-6)!} (0.8)^6 (0.2)^4 = 0.0881\]</p>
        
        <p><strong>f) What is the probability that the treatment cures toenail fungus in exactly 7 of the 10 patients?</strong></p>
        <p>We can use our probability formula to answer this question:</p>
        <p>\[P(7) = \binom{10}{7} (0.8)^7 (1-0.8)^{10-7} = \frac{10!}{7!(10-7)!} (0.8)^7 (0.2)^3 = 0.2013\]</p>
        
        <p><strong>g) What is the probability that the treatment does not cure toenail fungus in at least 1 of the 10 patients?</strong></p>
        <p>To answer this question, we will use the complement rule. The probability that the treatment does not cure toenail fungus in at least one of the patients is equal to one minus the probability that it cures toenail fungus in all of the patients. We can calculate the probability that the treatment cures toenail fungus in all of the patients as follows:</p>
        <p>\[P(10) = \binom{10}{10} (0.8)^{10} (1-0.8)^{10-10} = \frac{10!}{10!(10-10)!} (0.8)^{10} (0.2)^0 = (0.8)^{10} = 0.1074\]</p>
        <p>The probability that the treatment does not cure fungus in at least one of the patients is therefore: 1 − 0.1074 = <strong>0.8926</strong>.</p>
    </div>
    
   <h2>A Warning About the Binomial Distribution</h2>
    
    <p>Something to keep in mind when dealing with binomial distributions: <em>They are not always symmetric!</em> Unlike a normal distribution (which is symmetric, by definition), a binomial distribution can be symmetric, skewed left, or skewed right.</p>
    
    <p>The coin-flipping examples are symmetric binomial distributions, but they were symmetric only because the probability of a success was exactly 0.5. If that probability were to change, the shape of the distribution would change as well. The closer p is to zero or one (i.e., the farther away it is from 0.5), the more skewed the binomial distribution will be.</p>
    


</body>
</html>
